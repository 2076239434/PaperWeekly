# nnU-Net: Breaking the Spell on Successful Medical Image Segmentation

Fabian Isensee et al. German Cancer Research Center/University of Heidelberg/Heidelberg University Hospital

## 0.Abstract

Fueled by the diversity of datasets, semantic segmentation is a popular subfield in medical image analysis with a vast number of new methods being proposed each year. This ever-growing jungle of methodologies, however, becomes increasingly impenetrable. At the same time, many proposed methods fail to generalize beyond the experiments they were demonstrated on, thus hampering the process of developing a segmentation algorithm on a new dataset. Here we present nnU-Net (’no-new-Net’), a framework that automatically adapts itself to any given new dataset. While this process was completely human-driven so far, we make a first attempt to automate necessary adaptations such as preprocessing, the exact patch size, batch size, and inference settings based on the properties of a given dataset. Remarkably, nnU-Net strips away the architectural bells and whistles that are typically proposed in the literature and relies on just a simple U-Net architecture embedded in a robust training scheme. Out of the box, nnU-Net achieves state of the art performance on six well-established segmentation challenges. Source code is available at https://github.com/MIC-DKFZ/nnunet.

在多样化的数据集基础之上，语义分割是医学图像分析的一个流行子领域，每年都会提出很多新方法。这种一直在发展的情况，已经变得越来越难以理解。同时，很多提出的方法在展示的试验之外难以泛化，所以阻碍了在新数据集上提出新的算法。这里我们提出nnU-Net，这是一种可以在给定的新数据集之上可以自动调节的框架。目前这个过程还完全是基于人力驱动的，但我们第一次尝试将这种调节自动化，包括预处理、精确的图像块大小、批大小和推理设置，这都是基于给定数据集的性质的自动化。更显著的是，nnU-Net没有任何架构上的花哨，只是采用了简单的U-Net架构，而且采用了稳健的训练方案。nnU-Net在6个确定的分割挑战上得到了目前最好的性能。代码已开源。

**Keywords**: Medical Image Segmentation · U-Net · Generalization

## 1 Introduction

Semantic segmentation remains a popular research topic in the domain of medical image computing with 70% of the international competitions being devoted to it [8]. Important reasons for the enduring interest certainly are the diversity and individual peculiarities of imaging datasets encountered in the medical domain (see e.g. [14] for a detailed overview): datasets vary tremendously when considering cohort size, image dimensionality, image size, voxel intensity ranges and intensity interpretation. Class labels in the images can be highly imbalanced and labels can be ambiguous, while expert annotation quality varies strongly from dataset to dataset. Furthermore, certain datasets are quite inhomogeneous with respect to image geometries or might exhibit slice misalignments and extremely anisotropic spacings. Taken together, these circumstances make it more difficult to generalize findings from one task to others, and they often prevent immediate success when re-applying methods out of the box to a different problem. The process of adjusting design decisions or proposing new design concepts is complex: most choices are highly dependent on each other, and evidence to substantiate choices is spread over myriads of papers including a lot of ”noise”. This naturally led to a large number of segmentation methods being proposed in the recent years. Just to provide some prominent examples: variations of encoder-decoder style architectures with skip connections, first introduced by the U-Net [12], include the introduction of residual connections [9], dense connections [6], attention mechanisms [10], additional loss layers [5], feature recalibration [13], and others [11]. The specific modifications differ substantially from each other, but they all share a particular focus on architectural modifications. Given the large amount of segmentation-related publications on the one hand and the diversity of the specific implementations and dataset-related challenges on the other, it becomes increasingly difficult to follow the literature and ascertain which design principles actually generalize beyond the experiments they were demonstrated on. Based on our own experience, many new design concepts did not improve, or sometimes even worsened the performance of a well-designed baseline.

语义分割一直是医学图像计算中的流行研究话题，70%的国际竞赛都致力其中[8]。这种持续的兴趣的重要原因，当然是医学领域的图像数据集多样化又个性化（如[14]有详细的概述）：当考虑到数据集规模，图像维度，图像大小，体素灰度范围和灰度解释这众多因素时，数据集的差异性非常大。图像中的类别标签可能非常不均衡，标签也可能是模糊的，而专家标注质量在不同的数据集中差异也很大。而且，很多数据集在图像几何上都是不均一的，可能会出现切片对不齐和各向异性差异很大的情况。所有这些情况一起，这使得一篇文章的结果很难泛化到其他情况下，当直接将一些文章的结果应用到不同问题时，通常会导致不会直接成功。调整设计或提出新的设计概念的过程是复杂的：多数选项是高度互相依赖的，而证实这些选项的证据，是分布在很多paper里的，这些paper中有很多的噪声。这很自然的带来了，每年都会提出很多新的分割方法。举一些很明显的例子：带有skip连接的编码器-解码器类型架构的变体，首先由U-Net提出，后面有引入的残差连接，密集连接，注意力机制，额外的损失层，特征重校准，以及其他的。具体的修改各自非常不同，但它们在架构修改上都有一个共同的特点。有这么多基于分割的文章，还有很多基于数据集的挑战赛，要按照这些文献，确定哪些设计原则实际上可以在展示的数据集之外泛化，这是非常困难的事。根据我们自己的经验，很多新的设计概念并没有改进，甚至是使得一些设计很好的基准变得效果更差了。

A key problem in medical image computing today is that the process of applying a (segmentation) method to a new problem is completely human-driven. It is based on experience, with the papers mostly focusing on the network architecture, while merely brushing over all the other hyperparameters. Sub-optimal adaptations of a baseline method are regularly compensated for by the proposal of a new architecture. Since the strong dependencies and amount of local minima in hyperparameter space make it really hard to optimally adapt a method to a new problem, nobody in this loop can really be blamed. This situation can be frustrating for the researcher as well as the whole community. Especially in the medical imaging domain where datasets are so diverse, progress will largely depend on our ability to solve these problems.

今天，医学图像计算的一个关键问题是，将分割方法应用到新的问题上，这个过程完全是由人来推动的。这是基于经验的，大多数文章都是专注于网络架构，而很少有人梳理其他所有超参数。基准方法的次优改变一般都可以提出新的架构来进行补偿。因为超参数强烈互相依赖，而且存在很多局部最优点，这使其非常难于将一种方法改变用于新问题，所以这个循环中不能真的去怪罪于谁。这个情况对于研究者乃至对于整个团体来说都是很令人沮丧的。尤其是在医学图像领域，数据集非常多样化，这个过程主要还是依赖于，我们解决这些问题的能力。

This paper attempts a first step in this direction: we propose no-new-Net (nnU-Net), a segmentation method that includes a formalism for automatic adaptation to new datasets. Based on an automated analysis of the dataset, nnU-Net automatically designs and executes a network training pipeline. Being wrapped around the standard U-Net architecture [12], the hypothesis was that a systematic and careful choice of all hyperparameters will still yield competitive performance. Indeed, without any manual fine-tuning, the method achieves state-of-the-art performance on several well-known medical segmentation benchmarks.

这篇文章在这个方向上作出一个尝试：我们提出nnU-Net，这种分割方法可以自动对新的数据集进行调整。基于对数据集的自动化分析，nnU-Net自动设计并执行网络训练的流程。我们还是利用标准的U-Net架构，这个假设是，系统并仔细的选择所有超参数，还是可以得到非常好的性能的。确实，在没有任何手动精调的情况下，这个方法在几个著名的医学分割基准测试上得到了目前最好的结果。

## 2 Method

A segmentation algorithm can be formalized as a function $f_θ(x) = \hat y$, with x being an image, $\hat y$ the corresponding predicted segmentation and θ the set of hyperparameters required for training and applying the method. The dimensionality of θ can be quite large, covering the entire experimental pipeline from preprocessing to inference. Publications usually focus on reporting and substantiating the most relevant choices regarding θ, and ideally provide source code to cover θ entirely. This process, however, lacks insights into how θ must be adjusted if transitioning to a new dataset with different properties. Here, we make the first attempt at formalizing this process. Specifically, we seek for a function g(X, Y) = θ that generalizes well between datasets. As a first step, this requires identifying those hyperparameters that do not need adaptation, in our case reflecting a strong but simple segmentation architecture and a robust training scheme, and those that are dynamic, i.e. need to be changed in dependence of X and Y. In a second step we define g for the dynamic parameters, which in our case is a set of heuristics rules that adapt the normalization and resampling scheme, configure the patch size and batch size as well as compute the exact network geometries including ensembling and inference techniques. Taken together, this is nnU-Net, a segmentation framework that adapts itself without any user interaction to previously unseen datasets.

一个分割算法可以写成$f_θ(x) = \hat y$，其中x为图像，$\hat y$为对应的预测分割结果，θ为超参数集，需要θ训练并应用这种方法。θ的维度可以非常大，覆盖整个试验流程，从预处理到推理。很多文章只关注并证实θ里最有关的选项，理想情况下还会给出源码，以覆盖θ的所有情况。但是，这个过程缺少θ应当怎样调节的思想，如果迁移到一个新的数据集，数据集有新的性质，那么就不好调节了。这里，我们首次进行尝试，将这个过程规范化。具体的，我们寻找一个函数g(X, Y) = θ，可以在数据集之间泛化良好。第一步，这需要识别出那些不需要调整的超参数，在我们的情况中，就是一个简单但强壮的分割架构，一个稳健的训练方案，而哪些动态的东西，即，需要随着X和Y进行变化的。在第二步，我们定义g为动态参数，在我们的情况中，是一套启发式的规则，修改那些归一化和重采样方案，配置剪切块大小和批大小，以及计算精确的网络几何，包括集成和推理技巧。这些放到一起，就是nnU-Net，不需要用户交互就可以自动修改适应到之前没看到过的数据集的这样一个分割框架。

### 2.1 Preprocessing

**Image Normalization**. nnU-Net requires the information of what modality its input channels are. If a modality is not CT, nnU-Net normalizes intensity values by subtracting the mean and dividing by the standard deviation. If a modality is CT, all foreground voxels in the training set are collected and an automated level-window-like clipping of intensity values is performed based on the 0.5 and 99.5th percentile of these values. To conform with typical weight initialization methods, the data is then normalized with the global foreground mean and standard deviation. The described scheme is independently applied to each case and each modality.

**图像归一化**。nnU-Net需要输入通道的模态信息。如果一个模态不是CT，nnU-Net将其灰度值进行归一化，减去其均值，除以其标准差。如果一个模态是CT，训练集中所有的前景体素都收集起来，然后根据灰度值的0.5值和0.995值进行level-window-like自动截取。为与典型的权重初始化方法符合，数据然后根据全局前景均值和标准差进行归一化。这个方案对每个病例、每个模态都进行应用。

**Voxel Spacing**. nnU-Net collects all spacings within the training data and for each axis chooses the median as the target spacing. All training cases are then resampled with third order spline interpolation. Anisotropic spacing (here out-of-plane spacing three times larger than in-plane spacing) can give rise to interpolation artifacts. In this case, out-of-plane interpolation is done using nearest neighbor. For the corresponding segmentation labels, spline interpolation is replaced by resampling each label separately with linear interpolation.


**体素的间隔**。nnU-Net收集了所有训练数据的间隔，对每个轴选择中值作为目标间隔。然后所有的训练案例用三阶样条插值进行重采样。各向异性的间隔（这里异面间隔比平面内间隔要大三倍以上）会带来插值的伪影。在本例中，异面的插值是用最近邻方法进行的。对于对应的分割结果，样条插值替换为用线性插值分别对每个标签重新采样。

### 2.2 Training Procedure

**Network Architecture**. Three U-net models are configured, designed and trained indepentently of each other: a 2D U-Net, a 3D U-Net and a cascade of two 3D U-Net models where the first generates a segmentation at a low resolution which is subsequently refined by the second model. The only notable changes to the original U-Net architecture are the use of padded convolutions to achieve identical output and input shapes, instance normalization and Leaky ReLUs instead of ReLUs.

**网络架构**。我们配置、设计并训练了三个U-Net模型，相互独立：一个2D U-Net，一个3D U-Net，和两个3D U-Net的级联模型，这两个模型，首先是第一个在低分辨率上生成了一个分割，然后由第二个模型进行改善。对原始U-Net架构的唯一改变，是采用了补零卷积，以得到同样的输入和输出形状大小，使用了instance归一化和Leaky ReLUs而不是ReLUs。

**Network Hyperparameters**. Depending on the shape of the preprocessed training data, the specific instantiation of the U-Nets is adapted. Specifically, nnU-Net automatically sets the batch size, patch size and number of pooling operations for each axis while keeping the memory consumption within a certain budget (12 GB TitanXp GPU). Hereby, large patch sizes are favored over large batch sizes (with a minimum batch size of 2) to maximize the amount of spatial context that can be captured. Pooling along each axis is done until further pooling would reduce the spatial size of this axis below 4 voxels. All U-Net architectures use 30 convolutional filters in the first layer and double this number with each pooling operation. If the selected patch size covers less than 25% of the voxels in a typical training case, the 3D U-Net cascade is additionally configured and trained on a downsampled version of the training data. The cascade is intended to enable nnU-Net to still acquire sufficient context if the patch size is too small to cover the full resolution.

**网络超参数**。随着预处理过的训练数据的形状的不同，修改得到U-Nets的具体实例化。具体的，nnU-Net自动设置批大小，剪切块大小和每个轴的池化操作的数量，同时保持内存消耗在特定预算之下（TitanXp的12GB GPU之内）。以此方式，大的图像块大小通常要配合大的批次大小（最小批大小为2），以使能够捕获到的空间上下文最大化。在每个轴上的池化不断进行，直到再池化会使这个轴的空间大小降低到4以下。所有U-Net架构在第一层使用30个卷积滤波器，每次池化都将数量加倍。如果在典型的训练情况中，选择的剪切块大小覆盖的体素少于25%，那么级联3D U-Net就进行额外的配置，在训练数据的下采样版本中进行训练。级联的目的是使nnU-Net在如果剪切块很小，不能覆盖完整分辨率的情况下，仍然能够取得充分的上下文。

**Network Training**. All U-Net architectures are trained in a five-fold cross-validation. One epoch is defined as processing 250 batches. The sum of the cross-entropy loss and the dice loss are used as loss function. Adam was used as optimizer for stochastic gradient descent with an initial learning rate of 3 × 10^−4 and l2 weight decay of 3 × 10^−5. Whenever the exponential moving average of the training loss does not improve within the last 30 epochs the learning rate is dropped by a factor of 0.2. Training is stopped when the learning rate drops below 10^−6 or 1000 epochs are exceeded. We apply data augmentation on the fly during training using the batchgenerators framework . Specifically, we use elastic deformations, random scaling and random rotations as well as gamma augmentation. If the data is anisotropic, the spatial transformations are applied in-plane as 2D transformations.

**网络训练**。所有U-Net架构都采用五折交叉验证的方法进行训练。一个epoch定义为处理250批的数据。交叉熵损失和dice损失的和用作损失函数。Adam用作随机梯度下降的优化器，初始学习速率为3 × 10^−4，l2权重衰减为3 × 10^−5。只要训练损失的指数滑动平均在最近30个epochs没有下降，那么就将学习速率乘以0.2。当学习速率降低到10^−6以下，或超过了1000 epochs，那么训练就中止。我们在训练时，使用批生成器框架进行数据扩增。具体的，我们使用弹性变形，随机缩放和随机旋转，以及gamma扩充。如果数据是各向异性的，空间变换就作为2D变换在平面内进行。

### 2.3 Inference

Cases are predicted using a sliding window approach with half the patch size overlap between predictions. This increases the weight of the predictions close to the center relative to the borders. Test time data augmentation is applied by mirroring along all axes.

病例的预测使用的是滑窗方法，一半的剪切块大小在预测之间是重叠的。与边缘部分相比，这增加了靠近中间部分的预测的权重。测试时的数据扩增，使用的是沿着各个轴之间的镜像。

nnU-Net ensembles combinations of two U-Net configurations (2D, 3D and cascade) and based on the cross-validation results automatically chooses the best single model or ensemble to be used for test set prediction. For the selected configurations, nnU-Net furthermore uses the five models resulting from the cross-validation for ensembling.

nnU-Net将两个U-Net的配置进行集成（2D，3D和级联的），基于交叉验证结果，自动选择最好的单个模型或集成模型，以在测试集上进行预测。对于选择的配置，nnU-Net进一步使用交叉验证得到的五个模型进行集成。

## 3 Results

nnU-Net was initially developed on the seven training datasets of the phase I from the Medical Segmentation Decathlon challenge [4]. As described in [14], these datasets cover a substantial amount of variability and challenges that are typically encountered in medical segmentation problems. nnU-Net was evaluated both on the Medical Decathlon Segmentation challenge (phase I and phase II) as well as five additional popular medical segmentation challenges. All challenge participations are summarized below and an overview is given in Figure 1.

nnU-Net最初是在医学分割迪卡侬挑战赛第一阶段的7个训练数据集上提出来的。如[14]所述，这些数据集覆盖了众多的变化和挑战，这在医学分割问题中都是很常见和典型的。nnU-Net在迪卡侬医学分割挑战赛的第一阶段和第二阶段都进行了评估，在另外五个流行的医学分割挑战赛上也进行了评估。所有挑战赛的参与情况总结如下，概览如图1所示。

Fig.1. Summary of nnU-Net performance on the test sets of medical segmentation challenges. All leaderboard submissions are plotted for each dataset and label (accessed on March 25th 2019). Numbers for Decathlon, LiTS, ACDC and BCV are Dice scores, MS lesion and PROMISE12 use different metrics [3,7]. Best viewed in electronic format.

**Medical Segmentation Decathlon (Decathlon)**. Phase I of this challenge consisted of the seven aforementioned datasets which were used by participants to develop generalizable segmentation algorithms. In phase II, three additional datasets that were previously unknown were made available. Algorithms were applied to these datasets with no further changes or user interaction. The evaluation for both phases was done on the official test sets. nnU-Net was the clear winner of the Decathlon challenge in both phase I and phase II. **Automatic Cardiac Segmentation Challenge (ACDC)** [1]: Here, three parts of the heart were to be segmented in cine-MRI images for two different time steps. 100 training cases were provided with two time steps each. We manually split the data for nnU-Nets five fold cross-validation runs to ensure patient stratification. nnU-Net achieved the first place in the open leaderboard (based on the 50 test cases) and set a new state-of-the art on this dataset. **Longitudinal multiple sclerosis lesion segmentation challenge** [3]: The task was to segment MS lesions in MRI images. 5 patients were provided, each with 4-5 time points (21 time points in total) and two raters providing annotations per time point. We treated each rater as a separate training example and again manually split the training cases to ensure patient stratification. On the test set, nnU-Net ranked 5th out of 163 with a score of 93.09 and was just closely behind four submissions from Vanderbilt University of which the highest has a score of 93.21. **PROMISE12** [7]: The task was the segmentation of the prostate in anisotropic MRI images. 50 annotated training cases and 30 unlabelled test cases were provided. nnU-Net achieved a test set score of 89.08 which places it 11th out of a total of 290 submissions (1st place: 89.59). **LiTS**. The Liver Tumor Segmentation challenge [2] consists of 131 training images (CT) and 70 test cases. For the training cases, segmentations of the liver and liver tumors were provided. nnU-Net achieved dice scores for lesion and liver of 0.725 and 0.958, respectively. Post processing by removing all but the largest connected foreground region improved the dice scores to 0.738 and 0.960, setting a new state of the art in lesion dice while representing the 17th place for the liver label (first place: 0.966) on the open leaderboard (123 teams in total). **Multi-Atlas Labeling Beyond the Cranial Vault Challenge (Abdomen)**. Here the task was segmentation of 13 organs in abdominal CT images. The challenge provided 30 annotated training images and 20 test images. nnU-Net set a new state of the art with an average dice score of 88.1%, which is more than 3 points higher than the next best result (43 submission in total on the leaderboard). Looking at the individual organs, nnU-Net scored the highest dice scores in 11/13 organs.

**医学分割迪卡侬**。这个挑战的第一阶段包括之前提到的7个数据集，参与者用这些数据集开发可泛化的分割算法。在第二阶段，之前不知道的三个额外的数据集现在可用了。应用在这些数据集上的算法，不进行更多的改变或用户交互。两个阶段的评估都在官方的测试集上进行。nnU-Net在第一阶段和第二阶段中都是很明显的大赢家。**自动心脏分割挑战赛 (ACDC)**：这里，在cine-MRI图像中，对两个不同的时间步骤，对心脏的三个不同部分进行分割。在两个时间步骤中每个提供了100个训练案例。我们手动将数据进行分割，以对nnU-Net进行五折交叉验证，确保病人分了层。nnU-Net在公开的排行榜上取得了第一名的位置（基于50个测试案例），在数据集上树立了新的最好结果。**纵向多樱花节损伤分割挑战赛**：任务是在MRI图像中分割MS损伤。给出了5个病人，每个都有4-5个时间点（总计21个时间点），两个评估者在每个时间点给出标注。我们将每个评估者看作单独的训练样本，再次手动将训练集的案例分割以确保病人的分层。在测试集中，nnU-Net在163个模型中排名第5，分数为93.09，前面四个模型的分数也非常接近，最高93.21。**PROMISE12**：任务是在各向异性的MRI图像中分割前列腺。给出了50标注的训练样本和30个未标注的测试样本。nnU-Net在测试集上取得的分数是89.08，在全部290个参赛者中，我们排名第11，第一名的成绩是89.59。**LiTS**。肝部肿瘤分割挑战赛包含131个训练图像(CT)，和70个测试案例。对于训练样本，给出了肝部和肝部肿瘤的标注。nnU-Net对损伤和肝部的的dice分数分别为0.725和0.958。后处理采用的是去除除了最大连接的前景区域之外的所有区域，将dice改进到0.738和0.960，在公开排行榜上，确立了新的损伤dice最好标准，肝部分割排名第17（最好成绩0.966），共计123个小组。**多模版颅顶外标注挑战赛（腹部）**。这里的任务是分割腹部CT图像中的13个器官。挑战赛提供了30个标注的训练图像，和20个测试图像。nnU-Net树立了新的最好标准，dice分数为88.1%，比第二名高了3个点（排行榜上共有40支参赛队伍）。对单个器官进行比较，nnU-Net在13个器官中的11个上，都得到了最高的分数。

Figure 2 shows ablation studies that were conducted to confirm design choices made in nnU-Net. All experiments were done on one split of the training data and a representative subset of datasets from Phase I of the Decathlon was chosen. These results indicate on the one hand that our decision for using Leaky ReLUs should be revised, but on the other hand confirm our choice of instance normalization, data augmentation and loss function.

图2进行了一些分离试验，以对nnU-Net中的设计决策进行确认。所有的试验都是在训练集的一个split上的，选择了迪卡侬阶段1的数据集的一个代表性子集。这些结果说明，一方面，我们使用Leaky ReLUs的决定需要改变，但另外一方面，也确认了instance归一化，数据扩充和损失函数的选择。

## 4 Discussion

We introduce nnU-Net, a framework that automatically adapts itself to any given medical segmentation dataset without user intervention. To the best of our knowledge, nnU-Net is the first attempt at formalizing the neccessary adaptations that need to be made between datasets. nnU-Net achieves state of the art performance on six publicly available segmentation challenges. This is remarkable given that nnU-Net took away most of the more complicated developments of the recent years and relied on nothing but simple U-Net architectures. Most importantly, it has to be stressed that we did not manually tune hyperparameters between challenges and that all design choices were automatically determined by nnU-Net. It is thus even more surprising that it set new state of the art results in datasets where it specifically competed against manually tuned algorithms.

我们提出了nnU-Net，自动对任何给定的医学分割数据集进行调整的框架，而不需要用户干预。据我们所知，nnU-Net是第一种尝试，对不同数据集之间的差异进行自动调整。nnU-Net在6个公开可用的分割挑战上取得了目前最好的性能。这是非常了不起的，因为nnU-Net只依赖于最简单的U-Net架构，而没有这些年来对其作出了诸多复杂的改动。最重要的是，我们不用手动调整不同挑战赛上的超参数，所有设计决定都是自动由nnU-Net决定的。因此这是非常令人惊讶的，在这些数据集上，与特别为这些数据集手动调整的算法相比，取得了目前最好的结果。

nnU-Net revolves around both the selection of well-generalizing static design choices such as the U-Net architecture, the dice loss, data augmentation and ensembling, as well as a number of the dynamic design choices that are determined by a set of rules that ultimately reflect our segmentation expertise. Using such rules may however not be the best way to approach this problem. Given a larger number of segmentation datasets, future work could attempt to directly learn this from the properties of the datasets. While the specific properties of nnU-Net chosen for this publication result in strong segmentation performance across a number of datasets, we do not claim to have found the globally optimal configuration. In fact, looking at the ablation studies presented in Fig. 2, we see that the choice of replacing ReLUs with Leaky ReLUs did not impact performance and that our data augmentation scheme may not be ideal for all datasets. Post-processing should also be investigated further. Our results on LiTS suggest that a properly chosen postprocessing can be beneficial. Such a postprocessing could be automated by analysing the training data or by choosing schemes based on the cross-validation results. An attempt for such automation was part of the initial version of nnU-Net that was used for the Decathlon Challenge, but later discarded for being too conservative and not consistently improving results.

nnU-Net采用的是经典的泛化的很好的选择，如U-Net架构，dice损失，数据扩充和集成，以及一些动态决策，这些决策由一系列规则决定，最终反映了我们的分割专业知识。使用这种规则，对于这种问题，可能并不是最佳的方式。有这么多分割数据集，未来的工作可以直接从数据集中学习这些性质。本文中nnU-Net选择的具体性质，在多个数据集中得到了很好的分割性能，但我们并不认为已经找到了全局最优的配置。实际上，观察图2中给出的分离试验，我们看到，采用Leaky ReLU替换ReLU的选项，并没有改进性能，而且我们的数据扩充方案也并不是对所有数据集都是理想的。后处理也应当进一步研究。我们在LiTS上的结果说明，合适的选择后处理技术是有好处的。这样一种后处理可以通过分析训练数据，或基于交叉验证的结果选择方案来自动化实现。nnU—Net的初始版本，就是这样一种自动化的尝试，用在迪卡侬挑战赛上，但后来放弃了，因为太过保守，不能持续的改进结果。

Now that we have established what could be considered the strongest U-Net baseline to date, we can systematically evaluate more advanced network designs with respect to their generalizability as well as their performance gain relative to the plain architecture employed here. nnU-Net can thus not only be used as an out-of-the-box segmentation tool, but also as a strong U-Net baseline and as a platform for future segmentation-related publications. nnU-Net will be made publicly available upon publication at https://github.com/MIC-DKFZ/nnunet.

现在我们已经确定了最强的U-Net基准，我们可以系统的评估更高级的网络设计，比如与泛化性有关的，以及与这里采用的简单架构相比得到的性能提升。nnU-Net因此不仅可用于开箱即用的分割工具，而且也是一个很强的U-Net基准，用于未来的分割相关的文章。nnU-Net已开源。
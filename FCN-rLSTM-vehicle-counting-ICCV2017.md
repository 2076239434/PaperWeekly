# FCN-rLSTM: Deep Spatio-Temporal Neural Networks for Vehicle Counting in City Cameras

Shanghang Zhang et al. Carnegie Mellon University

## Abstract 摘要

In this paper, we develop deep spatio-temporal neural networks to sequentially count vehicles from low quality videos captured by city cameras (citycams). Citycam videos have low resolution, low frame rate, high occlusion and large perspective, making most existing methods lose their efficacy. To overcome limitations of existing methods and incorporate the temporal information of traffic video, we design a novel FCN-rLSTM network to jointly estimate vehicle density and vehicle count by connecting fully convolutional neural networks (FCN) with long short term memory networks (LSTM) in a residual learning fashion. Such design leverages the strengths of FCN for pixel-level prediction and the strengths of LSTM for learning complex temporal dynamics. The residual learning connection reformulates the vehicle count regression as learning residual functions with reference to the sum of densities in each frame, which significantly accelerates the training of networks. To preserve feature map resolution, we propose a Hyper-Atrous combination to integrate atrous convolution in FCN and combine feature maps of different convolution layers. FCN-rLSTM enables refined feature representation and a novel end-to-end trainable mapping from pixels to vehicle count. We extensively evaluated the proposed method on different counting tasks with three datasets, with experimental results demonstrating their effectiveness and robustness. In particular, FCN-rLSTM reduces the mean absolute error (MAE) from 5.31 to 4.21 on TRANCOS; and reduces the MAE from 2.74 to 1.53 on WebCamT. Training process is accelerated by 5 times on average.

本文中，我们提出了一种深度时空神经网络，可以从城市摄像头拍摄的低质量视频中按顺序对车辆计数。城市摄像头视频分辨率低，帧率低，遮挡多，视角广，这使得多数现有的方法都无法有效处理。为克服现有方法的局限，利用交通视频的时间信息，我们设计了一种新的FCN-rLSTM网络，进行车辆密度和数量的联合估计，新的网络将全卷积网络(FCN)和长短期记忆网络(LSTM)以残差学习的方式结合起来。这种设计利用了FCN进行像素级预测的能力，和LSTM学习复杂的时间变化的能力。残差学习连接将车辆计数回归的问题重新表述为从每帧的密度和之中学习残差函数的问题，这显著提高了网络训练的速度。为保持特征图分辨率，我们提出了一种将atrous卷积用于FCN并将不同卷积层的特征图综合到一起的Hyper-Atrous方法。FCN-rLSTM使得特征表示更加精细，可以得到从像素到车辆数量的可训练的端到端映射。我们对提出的方法在三个数据集中不同的计数任务中进行广泛的评估，实验结果表明了其有效性和稳健性。特别是，FCN-rLSTM将TRANCOS上的MAE从5.31降低到了4.21，将WebCamT上的MAE从2.74降到了1.53。训练过程平均加速了5倍。

## 1. Introduction 简介

Many cities are being instrumented with hundreds of surveillance cameras mounted on streets and intersections [22, 39, 38]. They capture traffic 24 hours a day, 7 days a week, generating large scale video data. Citycam videos can be regarded as highly versatile, being an untapped potential to develop many vision-based techniques for applications like traffic flow analysis and crowd counting. This paper aims to extract vehicle counts from streaming real-time video captured by citycams. Vehicle count is the number of vehicles in a given region of the road [23]. As shown in Figure 1, we select a region of fixed length in a video and count the number of vehicles in that region.

很多城市的街道和十字路口都安装了数以百计的监控摄像头[22,39,38]。他们每时每刻都在捕捉交通信息，生成了大量的视频数据。城市摄像头可以认为是非常多用途的工具，具有极大的潜力，可以开发出很多基于视觉技术的应用，如交通流量分析和人群计数。本文的目标是从城市摄像头的实时流媒体视频中提取出车辆计数。车辆计数是路段给定区域中的车辆数量[23]。如图1所示，我们选择了视频中固定长度的区域，计算这个区域中车辆的数量。

Figure 1. FCN-rLSTM network to count vehicles in traffic videos captured by city cameras. The videos have low frame rate, low resolution, high occlusion, and large perspective. FCN and LSTM are combined in a residual learning framework, leveraging the strengths of FCN for dense visual prediction and strength of LSTM for modeling temporal correlation. Video frames are input into FCN, and the output density maps are fed into a stack of LSTMs to learn residual functions with reference to the sum of densities in each frame. The global vehicle count is finally generated by summing the learned residual and the densities.

图1. FCN-rLSTM网络对城市摄像头捕捉的交通视频中的车辆进行计数。视频帧率低，分辨率低，遮挡程度高，视角广。FCN和LSTM以残差学习的结构结合在一起，利用了FCN进行视觉密度预测的能力，和LSTM对时间相关性进行建模的能力。视频帧输入到FCN中，输出的密度图输入到一串LSTMs中，以学习相对于每帧中的密度和的残差函数。全局车辆数量最终由学习到的残差和密度之和给出。

Vehicle counting is of great importance for many real-world applications, such as urban traffic management. Important as it is, Counting vehicles from city cameras is an extremely difficult problem faced with severe challenges (illustrated in Figure 1) due to network bandwidth limitations, lack of persistent storage, and privacy concerns. Publicly available citycam video is limited by: 1. Low frame rate, ranging from 1 fps to 0.3 fps. 2. Low resolution, including 352 × 240, 320 × 240 or 704 × 480. 3. High occlusion, especially in rush hours. 4. Large perspective, resulting in various vehicle scales. All these challenges make vehicle counting from citycam data very difficult.

车辆计数对于很多真实世界应用来说有非常重要，比如市区交通管理。尽管如此重要，从城市摄像头中进行车辆计数是一个非常困难的问题，面临着严重的挑战（如图1所示），如网络带宽限制，缺少持续存储，和隐私问题。公开可用的城市摄像头视频受限于：1.低帧率，从1fps到0.3fps；2.低分辨率，包括352 × 240, 320 × 240或704 × 480.；3.遮挡程度高，尤其是高峰期；4.视角大，导致各种尺度下的车辆都有。所有这些挑战都使得从城市摄像头中进行车辆计数非常困难。

The challenges of citycam videos preclude existing approaches to vehicle counting, which can be grouped into five categories: frame differencing based [41, 12], detection based [52, 40], motion based [37, 9, 10, 27], density estimation based [42], and deep learning based [47, 50, 30, 51, 2, 35, 19] methods. The first three groups of methods are sensitive to environment conditions and tend to fail in high occlusion, low resolution, and low frame rate videos. While density estimation approaches avoid detecting or tracking individual vehicles, they perform poorly in videos with large perspective and oversized vehicles. Though the low frame rate citycam video lacks motion information, vehicle counts of sequential frames are still correlated. Existing methods fail to account for such temporal correlation [30, 49, 15, 20, 11, 46]. Work [2] and [49] achieve state-of-the-art performance on animal counting and traffic counting, respectively, yet they fail to model the temporal correlation as an intrinsic feature of the surveillance video.

城市摄像头视频的挑战使得现有的方法无法进行车辆计数，可以分为以下5类：基于帧间差分的[41,12]，基于检测的[52,40]，基于运动的[37,9,10,27]，基于密度估计的[42]和基于深度学习的[47,50,30,51,2,35,19]。前三类方法对环境条件很敏感，在高度遮挡、低分辨率和低帧率的情况下算法会失效。而基于密度估计的方法避免了对单个车辆的检测或跟踪，在大视角和过大过小的车辆的视频中表现不好。虽然低帧率的城市摄像头视频缺少运动信息，帧序列的车辆计数仍然互相关联。现有的方法没有利用这些时间之间的关联[30,49,15,20,11,46]。[2]和[49]分别在动物计数和交通计数中得到了目前最好的效果，但他们没有将时域关系作为监控视频的内在特征。

To overcome these limitations, we propose a deep spatio-temporal network architecture to sequentially estimate vehicle count by combining FCN [25] with LSTM [18] in a residual learning framework (FCN-rLSTM). The FCN maps pixel-level features into vehicle density to avoid individual vehicle detection or tracking. LSTM layers learn complex temporal dynamics by incorporating nonlinearities into the network state updates. The residual connection of FCN and LSTM reformulates global count regression as learning residual functions with reference to the sum of densities in each frame, avoiding learning unreferenced functions and significantly accelerating the network training. FCN-rLSTM enables refined feature representation and a novel end-to-end optimizable mapping from image pixels to vehicle count. The framework is shown in Figure 1. Video frames are input into FCN, and the output density maps are fed into LSTMs to learn the vehicle count residual for each frame. The global vehicle count is finally generated by summing the learned residual and the densities.

为克服这些限制，我们提出了一种深度时空网络架构，将FCN[25]和LSTM[18]以残差学习的结构结合起来(FCN-rLSTM)，以按顺序估计车辆计数。FCN将像素特征映射到车辆密度，以避免对车辆个体进行检测或追踪。LSTM层通过将非线性信息融入网络状态更新中，学习了复杂的时域动态。FCN和LSTM的残差连接将全局计数回归问题重新表述为相对于每帧的密度和来学习残差函数，避免了学习未引用的函数，显著加速了网络训练。FCN-rLSTM使得特征表示更加精炼，可以从像素到车辆数量进行端到端的可优化映射。这种框架如图1所示。视频帧输入到FCN中，输出密度图送入LSTMs，以学习每帧的车辆计数残差。全局车辆计数最后通过对学习到的残差和密度相加得到。

The proposed FCN-rLSTM has the following novelties and contributions: 1.FCN-rLSTM is a novel spatio-temporal network architecture for object counting, such as crowd counting [47], vehicle counting [30], and penguin counting [2]. It leverages the strength of FCN for dense visual prediction and the strengths of LSTM for learning temporal dynamics. Such network can learn from more information. To the best of our knowledge, FCN-rLSTM is the first spatio-temporal network architecture with residual connection for object counting.

提出的FCN-rLSTM有如下创新点和贡献：1.FCN-rLSTM是一种新的时空网络架构，可以进行目标计数，比如人群计数[47]，车辆计数[30]和企鹅计数[2]。它利用了FCN进行视觉密度预测的能力和LSTM学习时域动态的能力。这种网络可以从更多信息中学习。据我们所知，FCN-rLSTM是第一个带有残差连接的时空网络架构来进行目标计数的。

2.The residual connection between FCN and LSTM is novel and significantly accelerates the training process by 5 times on average, as shown by our experiments. Though some recent work on other visual tasks [14, 44] also explored spatio-temporal networks, none of them combines FCN with LSTM, or has the residual connection between CNN and LSTM. FCN-rLSTM can be potentially applied to other visual tasks that both require dense prediction and exhibit temporal correlation.

2.FCN和LSTM的残差连接是新颖的，显著加速了训练过程，平均达到了5倍加速，我们的实验给出了这样的结果。虽然最近一些其他领域的工作[14,44]也在探索时空网络，他们都没有将FCN和LSTM结合起来，或将CNN和LSTM以残差的方式结合在一起。FCN-rLSTM可以用于其他同时需要密度预测和时间关系的视觉任务。

3.One challenge for FCN based visual tasks is the reduced feature resolution [8] caused by the repeated max-pooling and striding. To solve this problem, we propose a Hyper-Atrous combination to integrate atrous convolution [8] in the FCN and to combine feature maps of different atrous convolution layers. We then add a convolution layer after the combined feature volume with 1 × 1 kernels to perform feature re-weighting. The selected features both preserve feature resolution and distinguish better foreground from background. Thus, the whole network accurately estimates vehicle density without foreground segmentation.

3.基于FCN的视觉任务的一个挑战是降低的特征分辨率[8]，这是重复的max-pooling和striding导致的。为解决这个问题，我们提出了Hyper-Atrous结合，以在FCN中使用atrous convolution[8]，并将不同atrous卷积层的特征图结合起来。然后我们在叠加的特征体后增加了一个卷积层，卷积核为1×1大小，以进行特征重新赋权。选择的特征保持了特征分辨率，并可以很好的区分前景和背景。所以，整个网络在没有进行前景分割的情况下，可以准确的估计车辆密度。

4.We jointly learn vehicle density and vehicle count from end-to-end trainable networks improving the accuracy of both tasks. Recent object counting literature [50, 30, 51, 2] estimates directly the object density map and sum the densities over the whole image to get the object count. But such methods suffer from large error when videos have large perspective and oversized vehicles (big bus or big truck). Our proposed multi-task framework pursues different but related objectives to achieve better local optimal, to provide more supervised information (both vehicle density and vehicle count) in the training process, and to learn better feature representation.

4.我们从端到端可学习的网络中同时学习到了车辆密度和车辆数量，并改进了两种任务的准确率。最近的目标计数文献[50,30,51,2]直接估计目标的密度图，然后在整幅图像中将各个点密度值相加，得到目标数量。但这种方法在视频视角很大、有很多过大或过小的车辆（大型公共汽车或大型货车）时错误很大。我们提出的多任务框架追求不同但相关的目标，以得到更好的局部极值点，以在训练过程中给出更多监督的信息（车辆密度和车辆数量），以学习到更好的特征表示。

5.We present comprehensive experiments on three datasets covering different counting tasks, such as vehicle counting and crowd counting, to show generalization and substantially higher accuracy of FCN-rLSTM. On the TRANCOS dataset [30], we improve over state-of-the-art baseline methods, reducing the MAE from 5.31 to 4.21.

5.我们在三个数据集上进行了广泛的实验，包括不同的计数任务，如车辆计数，人群计数，展示了很好的泛化能力，以及更好的精度。在TRANCOS数据集[30]上，我们改进了基准方法的最好结果，MAE从5.31降低到了4.21。

The rest of paper is outlined as follows. Section 2 briefly reviews the related work for vehicle counting. Section 3 details the proposed FCN-rLSTM. Section 4 presents experimental results, and Section 5 concludes the paper.

本文剩余部分组织如下。第2节简要的回顾了车辆计数相关的工作，第3节详述了提出的FCN-rLSTM，第4节给出了实验性的结果，第5节进行了总结。

## 2. Related Work 相关的工作

In this section, we provide a brief review of related work on vehicle counting and LSTM for visual tasks. 本节中，我们简要的回顾了车辆计数的相关工作，和LSTM在视觉任务中的相关工作。

### 2.1. Vision-based Methods for vehicle counting 基于视觉的车辆计数方法

Vision-based approaches deal with camera data, which have low installation costs, bring little traffic disruption during maintenance, and provide wider coverage and more detailed understanding of traffic flow [28, 13, 21]. They can be divided into five categories: 基于视觉的方法处理摄像头数据，安装成本很低，维护过程中对交通影响小，覆盖面大，交通流的理解细节多[28,13,21]。这些方法可以分成5类：

1. **Frame differencing methods** count vehicles based on the difference between sequential frames and are easy to implement. They suffer with noise, abrupt illumination changes, and background changes [41, 12]. 帧差分方法基于序列帧图像间的差异进行车辆计数，容易实现，但受到噪声、突然的亮度变化和背景变化的影响大[41,12]。

2. **Detection based methods** [52, 40] detect individual vehicles in each frame and perform poorly in low resolution and high occlusion videos. 基于检测的方法[52,40]检测每帧中的车辆个体，在低分辨率和高遮挡的视频中表现很差。

3. **Motion based methods** [37, 9, 10, 27] count vehicles by tracking and tend to fail with citycam videos due to their low frame rate and lack of motion information. 基于运动的方法[37,9,10,27]通过跟踪对车辆计数，但在城市摄像头视频中，由于低帧率和缺少运动信息，经常会失败。

4. **Density estimation based methods** deal with the limitations of detection and motion based method by mapping the dense (pixel-level) image feature into object densities, avoiding detecting or tracking each object, as shown in Figure 2. Reference [42] casts the counting problem as estimating an image density whose integral over an image region gives the count of objects within that region. Object density is formulated as a linear transformation of each pixel feature, with a uniform weight vector applied to the whole image. This method suffers from low accuracy when the camera has large perspective and oversized vehicles occur. 基于密度估计的方法，通过将密度图像特征（像素级）映射到目标密度上，解决了基于检测和基于运动的方法的局限，避免了检测或跟踪每个目标，如图2所示。[42]将计数问题看作估计图像密度图的问题，密度图在整个图像区域上的积分就是区域中目标的数量。目标密度从每个像素特征的线性变换中计算得到，在整个图像上的权重矢量是统一的。这种方法在摄像头视角大，存在过大的车辆时，准确率很低。

5. **Deep learning based counting methods** have been developed recently [47, 50, 30, 51, 2, 34, 33, 45] that significantly improved counting performance. Work [47] applies CNN to output a 1D feature vector and fits a ridge regressor to perform the final density estimation. This work is not based on FCN and cannot perform pixel-wise dense prediction. Reference [30] is based on FCN, but it does not have deconvolutional or upsampling layers, resulting in the output density map being much smaller than the input image. Reference [2] jointly estimates the object density map and performs foreground segmentation, but it does not address the problem of large perspective and various object scales. 基于深度学习的计数方法提出的时间不长[47,50,30,51,2,34,33,45]，但显著改进了计数性能。工作[47]使用了CNN，输出了一个1D特征向量，用一个脊回归函数来进行最终的密度估计。这个工作没有基于FCN，也不能进行逐像素的密集预测。[30]基于FCN，但却没有解卷积或上采样层，得到的输出密度图比输入图像小很多。[2]估计了目标密度图，并同时进行前景分割，但没有解决大视角和多尺度目标的问题。

All existing methods fail to model the temporal correlation of vehicle count in traffic video sequential frames. 所有现有的方法都不能对交通视频序列帧中的车辆计数问题中的时间关联进行很好的建模。

### 2.2. LSTM for Visual Tasks

In recent years, several works attempt to combine CNN with recurrent neural networks (RNN) [4] to model the spatio-temporal information of visual tasks, such as action recognition [14, 3], video description [14], caption generation [36], and multi-label classification [44]. However, no existing work models the spatio-temporal correlation for object counting, especially by combining CNN/FCN with RNN/LSTM. Some work [29] explores new design of the internal LSTM architecture, but none of the existing works combined FCN with LSTM in a residual learning fashion. Work [48] regards the crowd flow map in a city as an image and build spatio-temporal networks to predict crowd flow. It does not apply RNN or LSTM networks to learn the temporal information; instead, it aggregates the output of three residual neural networks to model temporal dynamics. Thus such work is essentially multiple convolutional neural networks, rather than the combination of CNN and LSTM.

近年来，有几项工作试图将CNN与RNN[4]结合起来，以对视觉任务的时空信息进行建模，比如行为识别[14,3]，视频描述[14]，标题生成[36]和多标签分类[44]。但是，现有的工作都没有对目标计数中的时空关系进行建模，尤其是通过结合CNN/FCN和RNN/LSTM进行建模。一些工作[29]探索了LSTM内在架构的新设计，但现有工作都没有将FCN和LSTM以一种残差学习的方式进行结合。[48]将城市中的人群流图视为图像，构建时空网络以预测人群流，它并没有使用RNN或LSTM以学习时间信息，相反，它将三个残差神经网络的输出累计起来，对时间动态进行建模。所以这样的工作实际上就是多个卷积神经网络，而不是与CNN与LSTM的结合。

## 3. FCN-rLSTM for vehicle counting

As the low spatial and temporal resolution and high occlusion of citycam videos preclude existing detection or motion based methods for vehicle counting, we propose to apply FCN [25] to map the dense (pixel-level) feature into vehicle density and to avoid detecting or tracking individual vehicles. FCN based density estimation allows arbitrary input resolution and outputs vehicle density maps that are of the same size as the input image. Existing object counting literature [50, 30, 51, 2] estimates the object density map and directly sums the density over the whole image to get the object count. But such methods suffer from large error when the video has large perspective and oversized vehicles (big bus or big truck). Thus we propose the FCN-rLSTM network to jointly estimate vehicle density and vehicle count by connecting FCN with LSTM in a residual learning fashion. Such design leverages the strengths of FCN for pixel-level prediction and the strengths of LSTM for learning complex temporal dynamics. Counting accuracy is significantly improved by taking the temporal correlation of vehicle counts into account. However, it is not easy to train the combined FCN and LSTM networks. We further propose the residual connection of FCN and LSTM to accelerate the training process. The resulting end-to-end trainable network has high convergence rate and further improves the counting accuracy. In the following subsections, we will explain the proposed network architecture and high-light additional details.

由于城市摄像头视频在时间和空间上分辨率都很低，而且遮挡很多，这限制了现有的基于检测或运动的方法都不能进行车辆计数，我们提出使用FCN[25]将密集特征（像素级）映射为车辆密度，以避免对个体车辆进行检测或跟踪。基于FCN的密度估计输入图像分辨率任意，输出的车辆密度图与输入图像大小相同。现有的目标计数文献[50,30,51,2]估计目标密度图，并在整幅图像中对密度图进行求和，以得到目标数量。但这种方法在视频视角很大，有大型车辆（大型公共汽车或大型卡车）时误差很大。所以我们提出FCN-rLSTM网络，将FCN和LSTM以残差学习的方式连接，以同时估计车辆密度和车辆数量。这种设计利用了FCN进行像素级预测的能力，和LSTM学习复杂的时间动态的能力。在将车辆计数的时间关系考虑进去以后，计数准确率得到了显著的提高。但是，训练FCN+LSTM网络并不容易。我们进一步提出了FCN和LSTM的残差连接，以加速训练过程。得到的端到端可训练网络收敛速度快，进一步改进了计数准确率。在后面的小节中，我们解释提出的网络结构，并强调额外的细节。

### 3.1. FCN-rLSTM Model & Network Architecture

The network architecture with detailed parameters is shown in Figure 3, which contains convolution network, deconvolution network, hyper-atrous feature combination, and LSTM layers. Inspired by the VGG-net [32], small kernels of size 3 × 3 are applied to both convolution layers and deconvolution layers. The number of filter channels in the higher layers are increased to compensate for the loss of spatial information caused by max pooling.

网络架构和参数细节如图3所示，包括卷积网络，解卷积网络，hyper-atrous特征组合，和LSTM层。受VGG[32]启发，在卷积层和解卷积层中都使用了小的3 × 3卷积核。增加了更高层中的滤波器通道数量，以补偿max-pooling造成的空间信息损失。

To preserve feature map resolution, we develop hyper-atrous combination, where atrous convolution [8] is integrated into the convolution networks, and the feature maps after the second max-pooling layer and the atrous convolution layers are combined together into a deeper feature volume. Atrous convolution is proposed by work [8]; it amounts to filter upsampling by inserting holes between nonzero filter taps. It computes feature maps more densely, followed by simple bilinear interpolation of the feature responses back to the original image size. Compared to regular convolution, atrous convolution effectively enlarges the field of view of filters without increasing the number of parameters. After several atrous convolution layers, we combine the features from the second max-pooling layer and the atrous convolution layers. And then, after the combined feature volume, we add a convolution layer with 1×1 kernels to perform feature re-weighting to encourage the re-weighted feature volume to distinguish better foreground and background pixels. The combined and re-weighted feature volume is input of the deconvolution network that contains two deconvolution layers. At the top of the FCN, a convolution layer with 1×1 kernel acts as a regressor to map the features into vehicle density.

为保存特征图分辨率，我们提出了hyper-atrous组合，其中atrous卷积[8]整合进了卷积网络中，第二个max-pooling层的特征图和atrous卷积层组合到了一起，成为更深的特征体。Atrous卷积由[8]提出；通过在非零滤波器taps间插入孔洞，其发展成了上采样滤波，其计算特征图更加密集，随后跟着一个简单的特征双线性差值，响应就成为了原始图像大小。与常规卷积相比，atrous卷积有效的增大了滤波器的感受野，而没有增加参数数量。在几个atrous卷积层后，我们将第二个max-pooling层的特征，和atrous卷积层的特征组合起来。然后，在组合的特征体后，我们增加了1×1卷积核的卷积层，以进行特征重赋权，使得到的新权重能更好的区分前景和背景像素。组合并重新赋权的特征体是解卷积网络的输入，包括2个解卷积层。在FCN的最上层，1×1卷积核的卷积层起到回归器的作用，将特征映射为车辆密度。

To incorporate the temporal correlation of vehicle counts from sequential frames, we combine LSTM with FCN to jointly learn vehicle density and count. RNN maintains internal hidden states to model the dynamic temporal behavior of sequences. LSTM extends RNN by adding three gates to an RNN neuron: a forget gate $f_t$; an input gate $i_t$; and an output gate $o_t$. These gates enable LSTM to learn long-term dependencies in a sequence, and make it easier to be optimized. LSTM effectively deals with the gradient vanishing/exploding issues that commonly appear during RNN training [31]. It also contains cell activation vector $c_t$ and hidden output vector $h_t$. We reshape the output density map of FCN into a 1D vector $x_t$ and feed this vector into three LSTM layers. Each LSTM layer has 100 hidden units and is unrolled for a window of 5 frames. The gates apply sigmoid nonlinearities $σ_i, σ_f, σ_o$, and tanh nonlinearities $σ_c$, and $σ_h$ with weight parameters $W_{hi}, W_{hf}, W_{ho}, W_{xi}, W_{xf}$, and $W_{xo}$, which connect different inputs and gates with the memory cells, outputs, and biases $b_i, b_f$, and $b_o$. We define the commonly-used update equations [16]:

为利用序列帧中车辆数量的时间相关信息，我们将LSTM与FCN组合到一起，以同时学习车辆密度和数量。RNN保持内部的隐藏状态，以对序列的动态时序行为进行建模。LSTM通过对RNN神经元增加了三个门，拓展了RNN：遗忘门$f_t$，输入门$i_t$和输出门$o_t$。这些门使LSTM可以学习序列中的长期依赖，并使其易于优化。LSTM可以有效的解决在RNN训练中经常出现的梯度消失/爆炸问题[31]。LSTM还包含了单元激活向量$c_t$和隐藏输出向量$h_t$。我们将FCN的输出密度图的形状改变为一个1D向量$x_t$，并将这个向量送入三个LSTM层中。每个LSTM层包含100个隐藏单元，并记录了5帧的窗口信息。这些门使用sigmoid非线性$σ_i, σ_f, σ_o$，和tanh非线性$σ_c, σ_h$，权重参数为$W_{hi}, W_{hf}, W_{ho}, W_{xi}, W_{xf}, W_{xo}$，权重连接了不同的输入、门和记忆单元，输出和偏置$b_i, b_f, b_o$。我们定义了常用的更新公式[16]：

$$i_t = σ_i (x_t W_{xi} + h_{t−1} W_{hi} + w_{ci} ⊙ c_{t−1} + b_i)$$
$$f_t = σ_f (x_t W_{xf} + h_{t−1} W_{hf} + w_{cf} ⊙ c_{t−1} + b_f)$$
$$c_t = f_t ⊙ c_{t−1} + i_t ⊙ σ_c (x_t W_{xc} + h_{t−1} W_{hc} + b_c)$$(1)
$$o_t = σ_o (x_t W_{xo} + h_{t−1} W_{ho} + w_{co} ⊙ c_t + b_o)$$

To accelerate training, FCN and LSTM are connected in a residual learning fashion as illustrated in Figure 4. We take the sum of the learned density map over each frame as a base count, and feed the output hidden vector of the last LSTM layer into one fully connected layer to learn the residual between base count and final estimated count. Compared to the direct connection of FCN and LSTM, the residual connection eases the training process and increases counting accuracy.

为加速训练，FCN和LSTM以残差学习的方式连接，如图4所示。我们对每一帧学习到的密度图求和，作为基准计数，并将最后一个LSTM层输出的隐藏矢量送入一个全连接层，以学习基准计数和最终估计计数的残差。与FCN和LSTM的直接连接相比，残差连接缓解了训练过程，增加了计数准确率。

Figure 4. Comparison of (a) Direct connection of FCN and LSTM (FCN-dLSTM); (b) Residual connection of FCN and LSTM.

### 3.2. Spatio-Temporal Multi-Task Learning

The ground truth supervision for FCN-rLSTM includes two types of information: the pixel-level density map and the global vehicle count for each frame. Generation of these supervision depends on how the objects are annotated. If the center of each object is labeled as a dot d, the ground truth vehicle count for frame i is the total number of labeled dots. The ground truth density $F_i^0 (p)$ for each pixel p in image i is defined as the sum of 2D Gaussian kernels centered at each dot annotation covering pixel p:

FCN-rLSTM的真值监督包括两种信息：每一帧的像素级密度图和全局的车辆计数。生成这些真值监督要看目标是怎样标注的。如果每个目标的中心标注为点d，帧i中的真值车辆数量就是标注点的总数。图像i中每个像素p点的真值密度$F_i^0 (p)$定义为以每个标注点为中心的覆盖到像素点p的2D高斯核的和：

$$F_i^0 (p) = \sum_{d∈D_i} N(p;d,δ)$$(2)

where $D_i$ is the set of the dot annotations, d is each annotation dot, and δ of the Gaussian kernel is decided by the perspective map. If each object is annotated by a bounding box $B = (x_1 , y_1 , x_2 , y_2 )$, where $(x_1 , y_1)$ are the coordinates of the left top point and $(x_2, y_2)$ are the coordinates of the right bottom point, the ground truth vehicle count for frame i is the total number of bounding boxes in frame i. The center o of each bounding box B is: $o_x = (x_1 + x_2)/2, o_y = (y_1 + y_2)/2$. Then, the ground truth density $F_i^0 (p)$ for each pixel p in image i is defined as:

其中$D_i$是标注点的集合，d是每个标注点，高斯核的δ是由视角图决定的。如果每个目标是由边界框$B = (x_1 , y_1 , x_2 , y_2 )$标注的，其中$(x_1 , y_1)$是左上顶点的坐标，$(x_2, y_2)$是右下顶点的坐标，那么帧i中的真值车辆数量就是帧i中的边界框总数。每个边界框B的中心o为$o_x = (x_1 + x_2)/2, o_y = (y_1 + y_2)/2$，那么，帧图像i中每个像素点p的真值密度$F_i^0 (p)$为：

$$F_i^0 (p) = \sum_{o∈O_i} N(p;d,δ)$$(3)

where the parameter $O_i$ is the set of bounding box centers in frame i. δ of the Gaussian kernel is decided by the length of the bounding box. 其中参数$O_i$是帧i中边界框中心的集合，高斯核的δ由边界框的长度决定。

The FCN task is to estimate the pixel-level density map, and the LSTM task is to estimate the global vehicle count for each frame. These two tasks are jointly achieved by training the whole FCN-rLSTM network end-to-end. The vehicle density is predicted from the feature map by the last convolution 1 × 1 layer of the FCN. Euclidean distance is adopted to measure the difference between the estimated density and the ground truth. The loss function for density map estimation is defined as follows:

FCN的任务是估计像素级的密度图，LSTM的任务是估计每一帧的全局车辆数量。这两个任务通过对整个FCN-rLSTM网络进行端到端的训练而同时完成。车辆密度由FCN的最后一个1×1卷积层的特征图预测。利用了欧几里得距离来衡量估计的密度图和真值密度图的差异。密度图估计的损失函数定义如下：

$$L_D = \frac {1}{2N} \sum_{i=1}^N \sum_{p=1}^P ||F_i(p,Θ) - F_i^0(p)||_2^2$$(4)

where N is the batch size and $F_i (p)$ is the estimated vehicle density for pixel p in image i, and Θ is the parameter of FCN. The second task, global count regression, is learned from the LSTM layers including two parts: (i) base count: the integration of the density map over the whole image; (ii) residual count: learned by the LSTM layers. We sum the two to get the estimated vehicle count:

其中N是批大小，$F_i (p)$是图像i中像素p的估计车辆密度，Θ是FCN的参数。第二个任务，全局数量回归，是由LSTM层学习得到的，包括两部分：(i)基准数量：密度图在整个图像上的积分；(ii)残差数量：由于LSTM层学习得到。我们将这两项叠加，以得到估计的车辆数量：

$$C_i = G(F_i ; Γ, Φ) + \sum_{p=1}^P F_i (p)$$(5)

where $G(F_i ; Γ, Φ)$ is the estimated residual count, $F_i$ is the estimated density map for frame i, Γ is the learnable parameters of LSTM, and Φ is the learnable parameters of the fully connected layers. We hypothesize that it is easier to optimize the residual mapping than to optimize the original mapping. The loss of the global count estimation is:

其中$G(F_i ; Γ, Φ)$是估计的残差数量，$F_i$是帧i的估计密度图，Γ是LSTM可学习的参数，Φ是全连接层的可学习参数。我们假设，与优化原始映射相比，对残差映射优化更为容易。全局数量估计的损失是：

$$L_C = \frac {1}{2N} \sum_{i=1}^N (C_i - C_i^0)^2$$(6)

where $C_i^0$ is the ground truth vehicle count of frame i, $C_i$ is the estimated count of frame i. Then overall loss function for the network is defined as: 其中$C_i^0$是帧i的真值车辆数量，$C_i$是真i的估计数量。网络总体损失函数定义为：

$$L = L_D + λL_C$$(7)

where λ is the weight of the global count loss, and it should be tuned to achieve best accuracy. By simultaneously learning the two related tasks, each task can be better trained with much fewer parameters. 其中λ为全局数量损失的权重，需要调节以达到最佳准确率。通过同时学习这两个相关的任务，每个任务都可以在少很多参数的情况下得到更好的训练。

The loss function is optimized via batch-based Adam [24] and backpropagation. Algorithm 1 outlines the FCN-rLSTM training process. As FCN-rLSTM can adapt to different input image resolutions and variation of vehicle scales and perspectives, it is robust to different scenes.

损失函数通过批次Adam[24]算法和反向传播优化。算法1列出了FCN-rLSTM训练过程的过程。由于FCN-rLSTM可以适应不同的输入图像分辨率和车辆尺度的大小和视角，所以对不同的场景都非常稳健。

Algorithm 1: FCN-rLSTM Training Algorithm ...

## 4. Experiments 实验

In this session, we discuss experiments and quantitative results: 1. We first evaluate and compare the proposed methods with state-of-the-art methods on public dataset WebCamT [49]. 2. We evaluate the proposed methods on the public dataset TRANCOS [30]. 3. To verify the robustness and generalization of our model, we evaluate our methods on the public crowd counting dataset UCSD. [5].

在本节中，我们讨论了实验和量化结果：1. 我们首先在公开数据集WebCamT[49]上评估和比较了提出的方法与目前最好的方法。2. 我们在公开数据集TRANCOS[30]上评估了提出的方法。为验证我们模型的稳健性和泛化能力，我们在公开的人群计数数据集UCSD[5]上评估了我们的方法。

### 4.1. Quantitative Evaluations on WebCamT 在WebCamT数据集上的量化评估

WebCamT is a public dataset for large-scale city camera videos, which have low resolution (352 × 240), low frame rate (1 frame/second), and high occlusion. Both bounding box and vehicle count are available for 60, 000 frames. The dataset is divided into training and testing sets, with 45,850 and 14,150 frames, respectively, covering multiple cameras and different weather conditions.

WebCamT是一个大规模城市摄像头视频的公开数据集，分辨率低(352×240)，帧率低(1fps)，遮挡多。数据集有60000帧，每帧都有边界框和车辆数量。这个数据集分为45850帧训练集和14150帧测试集，包含多个摄像头和不同的天气条件。

Following the same settings in [49], we evaluate our method on the 14,150 test frames of WebCamT, which contains 61 videos from 8 cameras. These videos cover different scenes, congestion states, camera perspectives, weather conditions, and time of the day. The training set contains 45,850 frames with the same resolution, but from different videos. Both training and testing sets are divided into two groups: downtown cameras and parkway cameras. Mean absolute error (MAE) is employed for evaluation. For FCN-rLSTM, the weight of the vehicle count loss is 0.01. The learning rate is initialized by 0.0001 and adjusted by the first and second order momentum in the training process. To test the efficacy of the proposed Hyper-Atrous combination, combination of FCN and LSTM, and the residual connection, we evaluate different configurations of FCN-rLSTM as shown in Table 1. Atrous indicates the atrous convolution; Hyper indicates hypercolumn combination of the feature maps; Direct connect indicates combining FCN with LSTM directly; Residual connect indicates connecting FCN with LSTM in residual fashion.

与[49]中的设置相同，我们在WebCamT的14150测试帧上评估我们的方法，这是8个摄像头的61段视频。这些视频包含不同的场景、堵车情况、摄像头视角和天气条件、时段。训练集包含45850帧图像，分辨率相同，但是不同的视频片段。训练集和测试集都分成两组：市中心的摄像头和郊区道路的摄像头。采用MAE来进行评估。对于FCN-rLSTM，车辆数量损失的权重为0.01，学习率开始设定为0.0001，根据训练过程的一阶和二阶动量来调节。为测试提出的Hyper-Atrous组合、FCN和LSTM的组合以及残差连接的效用，我们评估了FCN-rLSTM的不同配置，如表1所示。Atrous表示atrous卷积，Hyper表示特征图的hypercolumn组合，Direct connect表示直接将FCN和LSTM组合起来，Residual connect表示FCN和LSTM以残差方式连接到一起。

Table 1. Different configurations of FCN-rLSTM

Configuration | Atrous | Hyper | Direct connect | Residual Connect
--- | --- | --- | --- | ---
FCN-A | y | n | n | n
FCN-H | n | y | n | n
FCN-HA | y | y | n | n
FCN-dLSTM | y | y | y | n
FCN-rLSTM | y | y | n | y

**Data augmentation**. To make the model more robust to various cameras and weather conditions, several data augmentation techniques are applied to the training images: 1. horizontal flip, 2. random crop, 3. random brightness, 4. and random contrast. More details can be found in the released code and other data augmentation techniques can also be applied.

**数据扩充**。为使模型对不同的摄像头和天气条件更稳健，对训练图像使用了几种数据扩充的方法：1.水平翻转，2.随机剪切，3.随机亮度，4.随机对比度。更多细节可以在放出的代码中找到，当然也可以使用其他的数据扩充技术。

**Baseline approaches**. We compare our method with three methods: Baseline 1: Learning to count [42]. This work maps each pixel’s feature into object density with uniform weight for the whole image. For comparison, we extract dense SIFT features [26] for each pixel using VLFeat [43] and learn the visual words. Baseline 2: Hydra[30]. It learns a multi-scale non-linear regression model that uses a pyramid of image patches extracted at multiple scales to perform final density prediction. We train Hydra 3s model on the same training set as FCN-rLSTM. Baseline 3: FCN [49]. It develops a deep multi-task model to jointly estimate vehicle density and vehicle count based on FCN. We train FCN on the same training set as FCN-rLSTM.

**基准方法**。我们将我们的方法与三种方法进行了比较：基准1：Learning to count[42]。这个工作将每个像素的特征映射到目标密度，对整个图像采用相同的权值。为进行对比，我们对每个像素采用VLFeat[43]计算密集SIFT特征[26]，然后学习视觉词。基准2：Hydra[30]。这种方法学习了一种多尺度非线性回归模型，使用的是图像块的多尺度金字塔，进行最终的密度预测。我们训练Hydra 3s模型使用的训练集与FCN-rLSTM相同。基准3：FCN[49]。这种方法提出了一种基于FCN的多任务模型，可以同时估计车辆密度和车辆数量。我们训练FCN的训练集与FCN-rLSTM相同。

**Experimental Results**. We compare the error of the proposed and baseline approaches in Table 2. From the results, we see that FCN-rLSTM outperforms all the baseline approaches and all the other configurations. As the testing data cover different congestion states, camera perspectives, weather conditions, and time of the day, these results verify the efficacy and robustness of FCN-rLSTM. To do ablation analysis of the proposed techniques, we also evaluate the performance of different configurations as shown in Table 2. With the Hyper-Atrous combination, FCN-HA itself already outperforms all the baseline methods and achieves better accuracy than FCN-A and FCN-H, which verifies the efficacy of the Hyper-Atrous combination. FCN-rLSTM has higher accuracy than FCN-HA and FCN-dLSTM, which verifies the efficacy of the residual connection of FCN and LSTM. Figure 5 and Figure 6 compare the counting results of FCN-HA and FCN-rLSTM, from which we conclude that FCN-rLSTM estimates better the vehicle count (blue dashed circles) and reduces large counting error induced by oversized vehicles (purple dashed circles). Figure 8 shows the density map learned from FCN-rLSTM. Without foreground segmentation, the learned density map can still distinguish background from foreground in both sunny, rainy and cloudy, dense and sparse scenes. Figure 9 shows the counting results for six different cameras from downtown and parkway. The camera positions are shown in the map of Figure 7. From the counting curves, we see that the proposed FCN-rLSTM accurately counts the vehicles for multiple cameras and long time sequences.

**实验结果**。我们在表2中比较了提出的方法与基准方法的误差。从结果中可以看到，FCN-rLSTM超过了所有的基准方法和所有其他配置的算法。由于测试数据包含了不同的堵车状态、摄像机视角、天气情况和时段，这些结果验证了FCN-rLSTM算法的有效性和稳健性。为对提出的算法进行分离实验分析，我们还评估了算法在不同配置下的表现，如表2所示。在Hyper-Atrous的组合下，FCN-HA算法已经超过了所有的基准方法，比FCN-A和FCN-H取得了更好的结果，这也验证了Hyper-Atrous组合的有效性。FCN-rLSTM比FCN-HA和FCN-dLSTM有更高的准确率，这验证了FCN和LSTM的残差连接的有效性。图5和图6比较了FCN-HA和FCN-rLSTM的计数结果，从中可以得出结论，FCN-rLSTM估计得到的车辆计数结果更好（蓝色虚线圆），降低了由于过大的车辆造成的大的计数错误（紫色虚线圆）。图8给出了FCN-rLSTM学习得到的密度图。在没有前景分割的情况下，学习得到的密度图仍然可以区分出前景和背景，而且是在晴天、雨天和多云天，密集和稀疏等各种场景下。图9给出了6个不同摄像头的计数结果，包括市区的和郊区的。摄像头位置如图7所示。从计数曲线中，我们可以看出，提出的FCN-rLSTM算法，对多个摄像头的长时间视频序列，都准确的进行了车辆计数。

Table 2. Results comparison on WebCamT

Method | Downtown | Parkway
--- | --- | ---
Baseline 1 | 5.91 | 5.19
Baseline 2 | 3.55 | 3.64
Baseline 3 | 2.74 | 2.75
FCN-A | 3.07 | 2.75
FCN-H | 2.48 | 2.30
FCN-HA | 2.04 | 2.04
FCN-dLSTM | 1.80 | 1.82
FCN-rLSTM | 1.53 | 1.63

Figure 7. Test cameras in the urban area

Figure 8. Estimated density map for multiple cameras. Column direction: The first four cameras are from downtown, and the last two cameras are from parkway. Row direction: The first two rows are estimated density maps for cloudy frames; the middle two rows are for sunny frames; and the last two rows are for rainy frames. Better view in color. Some density values may be too small to be clearly seen.

Figure 9. Counting results for multiple cameras.

Besides the high accuracy achieved by FCN-rLSTM, the convergence of the proposed approach is also improved significantly. As shown in Figure 10, the FCN-rLSTM converges much faster than FCN alone networks (FCN-HA). The residual connection of FCN and LSTM also enables faster convergence than the direct connection.

除了FCN-rLSTM取得的高准确度，提出的方法的收敛性也得到了显著改进。如图10所示，FCN-rLSTM收敛速度比FCN-HA快的多。FCN和LSTM的残差连接也比直接连接情况下收敛的更快。

Figure 10. Convergence of FCN-HA, FCN-dLSTM, and FCN-rLSTM for: (a) Parkway Cameras (b) Downtown Cameras. Shading shows the MAE over Epochs and dark lines indicate the smoothed trend.

### 4.2. Quantitative Evaluations on TRANCOS 在TRANCOS数据集上的定量评估

We also evaluate the proposed method on a public dataset TRANCOS [30] to verify its efficacy. TRANCOS is a collection of 1244 images of different traffic scenes from surveillance camera videos. It has 46796 annotated vehicles in total and provides a region of interest (ROI) for each image. Images of TRANCOS are from very different scenarios and no perspective maps are provided. The ground truth vehicle density maps are generated by the 2D Gaussian Kernel in the center of each annotated vehicle [17].

我们还在公开数据集TRANCOS[30]上评估了提出的算法。TRANCOS从监控摄像头视频中收集了1244幅不同的交通场景图像，共计有46796个标注的车辆，给每个图像都指定了感兴趣区域ROI。TRANCOS中的图像是不同场景下的，并没有提供视角图。真值车辆密度图是由每个标注的车辆中心的2D高斯核来生成的[17]。

The MAE of the proposed method and baseline methods are compared in Table 3. Baseline 2-CCNN is a basic version of the network in [30], and Baseline 2-Hydra augments the performance by learning a multiscale regression model with a pyramid of image patches to perform the final density prediction. All the baselines and proposed methods are trained on 823 images and tested on 421 frames following the separation in [30]. From the results, we can see FCN-HA significantly decreases the MAE from 10.99 to 4.21 compared with Baseline 2-Hydra, and decreases the MAE from 5.31 to 4.21 compared with Baseline 3. As the training and testing images of TRANCOS are random samples from different cameras and videos, they lack consistent temporal information. Thus FCN-rLSTM cannot learn temporal patterns from the training data. The performance of FCN-rLSTM is not as good as FCN-HA, but it already outperforms all the baseline methods. When applying our proposed model to other datasets, we can choose the FCN-rLSTM configuration for datasets that have temporal correlation and choose the FCN-HA configuration for datasets that do not have temporal correlation. Figure 11 compares the estimated counts from the proposed and baseline methods. The estimated counts of the proposed methods are evidently more accurate than that of the baseline methods. FCN-rLSTM and FCN-HA have comparable estimation accuracy of vehicle counts.

我们提出的方法和基准方法的MAE在表3中进行了比较。基准2-CCNN是[30]中网络的基本版本，基准2-Hydra通过学习了一个带有图像块金字塔的多尺度回归模型以进行最终的密度预测，取得了更好的性能。所有的基准和提出的方法都在823幅图像上进行训练，在421幅图像上进行测试，这种分割是[30]中的方法。从结果中可以看到，FCN-HA显著降低了MAE，从基准2-Hydra的10.99降到了4.21，与基准3相比，MAE从5.31降到了4.21。由TRANCOS的训练和测试图像是不同摄像头和视频的随机样本，他们缺少连续的时域信息。所以FCN-rLSTM不能从训练数据中学习到时域模式。FCN-rLSTM的性能没有FCN-HA好，但已经超过了所有其他的基准方法。当把我们提出的模型应用于其他数据集时，我们可以对有时间关联的数据集使用FCN-rLSTM配置，对没有时域关联的数据集使用FCN-HA配置。图11比较了提出的方法和基准方法的数量估计结果。提出方法的估计结果明显比基准方法要好很多。FCN-rLSTM和FCN-HA对车辆计数有着类似的估计准确率。

Table 3. Results comparison on TRANCOS dataset

Method | MAE | Method | MAE
--- | --- | --- | ---
Baseline 1 | 13.76 | Baseline 3 | 5.31
Baseline 2-CCNN | 12.49 | FCN-HA | 4.21
Baseline 2-Hydra | 10.99 | FCN-rLSTM | 4.38

Figure 11. Results comparison on TRANCOS dataset.

### 4.3. Quantitative Evaluations on UCSD Dataset 在UCSD数据集上的定量评估

To verify the generalization and robustness of our proposed methods in different counting tasks, we also evaluate and compare our methods with baselines on the pedestrian counting dataset UCSD [5]. This dataset contains 2000 frames chosen from one surveillance camera. The frame size is 158 × 238 and frame rate is 10fps. Average number of people in each frame is around 25. The dataset provides the ROI for each video frame. By following the same setting in [5], we use frames from 601 to 1400 as training data, and the remaining 1200 frames as test data. Table 4 shows the results of our methods and existing methods, from which we can see that FCN-rLSTM outperforms all the baseline methods and the FCN-HA configuration. These results show our proposed methods are robust to other type of counting tasks.

为验证我们提出方法在不同计数任务中的泛化能力和稳健性，我们还在行人计数数据集UCSD[5]上评估了我们的方法，并与基准进行了比较。这个数据集包含从监控摄像头选取的2000帧图像，帧大小为158×238，帧率为10fps。每帧的平均人数为25左右。数据集为每帧给出了感兴趣区域ROI。与[5]中的设置相同，我们使用601-1400帧进行训练，剩余的1200帧进行测试。表4给出了我们的方法和现有方法的区别，从中可以看出FCN-rLSTM超过了所有基准方法和FCN-HA配置。这些结果说明了我们提出的方法对其他类型的计数任务的泛化能力是不错的。

Table 4. Results comparison on UCSD dataset

Method | MAE | MSE
--- | --- | ---
Kernel Ridge Regression [1] | 2.16 | 7.45
Ridge Regression [7] | 2.25 | 7.82
Gaussian Process Regression [5] | 2.24 | 7.97
Cumulative Attribute Regression [6] | 2.07 | 6.86
Cross-scene DNN[47] | 1.6 | 3.31
Baseline 3 | 1.67 | 3.41
FCN-HA | 1.65 | 3.37
FCN-rLSTM | 1.54 | 3.02

## 5. Discussion & Conclusion

Vehicle counting is of great significance for many real world applications. Counting vehicles from citycams is very challenging as videos from citycams have low spatial and temporal resolution, and high occlusion. To overcome these challenges, we propose a novel FCN-rLSTM network architecture to jointly estimate vehicle density and vehicle count by connecting FCN with LSTM in a residual learning fashion. Extensive evaluations on different counting tasks and three datasets demonstrate the effectiveness and robustness of the proposed methods. One limitation for FCN-rLSTM is that the window size of the unrolled sequential frames is restricted by the available memory capacity. Thus we cannot learn very long term temporal information from the current FCN-rLSTM architecture. This limitation does not significantly affect the counting performance, for small window sizes (five frames in this paper) is capable of learning the smoothness of vehicle count.

车辆计数对很多真实世界应用来说非常重要。从城市摄像头中进行车辆计数非常有挑战性，因为城市摄像头的视频时间和空间分辨率都很低，遮挡情况还严重。为克服这些挑战，我们提出一种新的FCN-rLSTM网络架构，将FCN和LSTM以残差的形式进行连接，同时估计车辆密度和车辆数量。在三个数据集上的不同计数任务上的广泛评估，表明了我们方法的有效性和稳健性。FCN-rLSTM的一个限制是相关的序列帧的窗口大小受到可用内存的限制。所以我们不能学习非常长期的时域信息。这个限制并没有明显影响计数表现，因为小的窗口（本文中是5）可以学习车辆计数的平滑性。
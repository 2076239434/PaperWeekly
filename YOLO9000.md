# YOLO 9000: Better, Faster, Stronger

Joseph Redmon, Ali Farhadi University of Washington, Allen Institute for AI

## Abstract 摘要

We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster R-CNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don’t have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.

我们提出了YOLO9000，一种目前最好的实时目标检测系统，可以检测超过9000个目标类别。我们先是提出YOLO检测方法的多种改进，有利用之前别人的工作，也有全新的改进。改进的模型，称为YOLOv2，在标准的检测任务如PASCAL VOC和COCO中，是目前最好的。使用了一种新的多尺度训练方法后，同样的YOLOv2模型可以在多种图像大小上运行，在速度和准确率上可以很容易的均衡。在VOC2007上，YOLOv2以67FPS的速度得到了76.8%的mAP。如果是40FPS的速度，那么YOLOv2可以达到78.6%的mAP，超过了目前最好的方法，如使用ResNet的Faster R-CNN和SSD，但速度要快的多。最后我们提出了一种方法，可以同时训练目标检测任务和分类任务。使用这种方法，我们在COCO检测数据集和ImageNet分类数据集上同时训练了YOLO9000。YOLO9000在ImageNet检测验证集上得到了19.7%的mAP，尽管只有200类数据中的44类检测数据。没在COCO中的156类，YOLO9000得到了16.0%的mAP。但YOLO不仅仅能检测200类，它能检测超过9000类的目标，而且仍然是以实时的速度运行的。

## 1. Introduction 引言

General purpose object detection should be fast, accurate, and able to recognize a wide variety of objects. Since the introduction of neural networks, detection frameworks have become increasingly fast and accurate. However, most detection methods are still constrained to a small set of objects.

通用目标检测应当是快速准确的，并可以识别很多类目标。由于神经网络的引入，检测框架已经越来越快越来越准确。但是，多数检测方法仍然局限于少数类别的目标。

Current object detection datasets are limited compared to datasets for other tasks like classification and tagging. The most common detection datasets contain thousands to hundreds of thousands of images with dozens to hundreds of tags [3] [10] [2]. Classification datasets have millions of images with tens or hundreds of thousands of categories [20] [2].

目前的目标检测数据集与其他任务的数据集相比起来还不太多，如分类和tagging。最常用的检测数据集包括数千到几十万幅图像，数十到数百个标签[3,10,2]。分类数据集则有数百万幅图像，数万到数十万个类别[20,2]。

We would like detection to scale to level of object classification. However, labelling images for detection is far more expensive than labelling for classification or tagging (tags are often user-supplied for free). Thus we are unlikely to see detection datasets on the same scale as classification datasets in the near future.

我们希望检测数据集能够达到分类数据集的水平。但是，标注检测用的图像比标注分类或tagging用的图像要复杂的多（tag经常是用户免费提供的）。所以检测数据集不太可能达到分类数据集的规模。

We propose a new method to harness the large amount of classification data we already have and use it to expand the scope of current detection systems. Our method uses a hierarchical view of object classification that allows us to combine distinct datasets together.

我们提出一种新方法来利用这些已有的大量分类数据，使其扩展目前的检测系统的视野。我们的方法使用层次化的目标分类视图，使我们可以将不同的数据集综合在一起。

We also propose a joint training algorithm that allows us to train object detectors on both detection and classification data. Our method leverages labeled detection images to learn to precisely localize objects while it uses classification images to increase its vocabulary and robustness.

我们还提出了一种联合训练算法，能同时在检测数据集和分类数据集上训练目标检测器。我们的方法利用标记的检测图像来学习精确定位目标，而同时利用分类图像来增加词汇量和稳健性。

Using this method we train YOLO9000, a real-time object detector that can detect over 9000 different object categories. First we improve upon the base YOLO detection system to produce YOLOv2, a state-of-the-art, real-time detector. Then we use our dataset combination method and joint training algorithm to train a model on more than 9000 classes from ImageNet as well as detection data from COCO.

使用这种方法，我们训练了YOLO9000，这是一种可以检测超过9000类目标的实时检测器。首先我们改进了YOLO检测系统，形成了YOLOv2算法，目前最好的实时检测器。然后我们利用数据集合并方法和联合训练算法，在ImageNet中的超过9000类数据和COCO的检测数据上训练模型。

All of our code and pre-trained models are available online at http://pjreddie.com/yolo9000/. 我们的所有代码和预训练模型都在网上公开。

## 2. Better

YOLO suffers from a variety of shortcomings relative to state-of-the-art detection systems. Error analysis of YOLO compared to Fast R-CNN shows that YOLO makes a significant number of localization errors. Furthermore, YOLO has relatively low recall compared to region proposal-based methods. Thus we focus mainly on improving recall and localization while maintaining classification accuracy.

YOLO与目前最好的检测系统相比，有很多缺点。YOLO的错误分析与Faster R-CNN相比表明，YOLO的定位错误明显更多。而且，与基于候选区域的方法相比，YOLO的召回率较低。所以我们主要改善召回率和定位准确度，同时保持分类准确率。

Computer vision generally trends towards larger, deeper networks [6] [18] [17]. Better performance often hinges on training larger networks or ensembling multiple models together. However, with YOLOv2 we want a more accurate detector that is still fast. Instead of scaling up our network, we simplify the network and then make the representation easier to learn. We pool a variety of ideas from past work with our own novel concepts to improve YOLO’s performance. A summary of results can be found in Table 2.

计算机视觉的趋势一般是网络越来越大、越来越深[6,18,17]。更好的性能一般取决于训练更大的网络或集成多个模型一起。但是，我们希望YOLOv2更加准确的同时，保持高速。我们没有增加网络的规模，而是简化网络，使表示更容易学习。我们吸取了之前工作的很多思想，提出我们新的想法，来改进YOLO的性能。表2给出了结果的概要。

**Batch Normalization**. Batch normalization leads to significant improvements in convergence while eliminating the need for other forms of regularization [7]. By adding batch normalization on all of the convolutional layers in YOLO we get more than 2% improvement in mAP. Batch normalization also helps regularize the model. With batch normalization we can remove dropout from the model without overfitting.

**批归一化**。批归一化可以明显改善收敛，同时减少对其他正则化形式的需要[7]。对YOLO中的所有卷积层加入批归一化，mAP提升了2%。批归一化也对模型有正则化作用。有了批归一化，可以直接从模型中去除dropout层，而且不会过拟合。

**High Resolution Classifier**. All state-of-the-art detection methods use classifier pre-trained on ImageNet [16]. Starting with AlexNet most classifiers operate on input images smaller than 256 × 256 [8]. The original YOLO trains the classifier network at 224 × 224 and increases the resolution to 448 for detection. This means the network has to simultaneously switch to learning object detection and adjust to the new input resolution.

**高分辨率分类器**。目前最好的所有检测方法都使用在ImageNet上预训练的分类器[16]。从AlexNet开始，多数分类器的输入图像都小于256×256[8]。原版YOLO以224×224大小的图像训练分类器网络，然后将分辨率增大为448×448以进行检测。这意味着网络需要同时学习目标检测，而且适应新的输入分辨率。

For YOLOv2 we first fine tune the classification network at the full 448 × 448 resolution for 10 epochs on ImageNet. This gives the network time to adjust its filters to work better on higher resolution input. We then fine tune the resulting network on detection. This high resolution classification network gives us an increase of almost 4% mAP.

对于YOLOv2，我们首先在448×448的分辨率上精调分类网络，在ImageNet上运行了10轮训练。这使网络调整了其滤波器，在高分辨率输入上可以表现更好。然后我们对检测任务精调了得到的网络。高分辨率分类网络使mAP提升了近4%。

**Convolutional With Anchor Boxes**. YOLO predicts the coordinates of bounding boxes directly using fully connected layers on top of the convolutional feature extractor. Instead of predicting coordinates directly Faster R-CNN predicts bounding boxes using hand-picked priors [15]. Using only convolutional layers the region proposal network (RPN) in Faster R-CNN predicts offsets and confidences for anchor boxes. Since the prediction layer is convolutional, the RPN predicts these offsets at every location in a feature map. Predicting offsets instead of coordinates simplifies the problem and makes it easier for the network to learn.

**带锚框卷积**。YOLO直接用卷积特征提取器上的全连接层来预测边界框的坐标。Faster R-CNN没有直接预测坐标，而是使用手工选取的先验来预测边界框[15]。Faster R-CNN中的RPN只使用卷积层来预测锚框的置信度和偏移。由于预测是用的卷积层，RPN在特征图的每一个位置都预测这些偏移。预测偏移，而不是预测坐标，这简化了问题，使网络更容易学习。

We remove the fully connected layers from YOLO and use anchor boxes to predict bounding boxes. First we eliminate one pooling layer to make the output of the network’s convolutional layers higher resolution. We also shrink the network to operate on 416 input images instead of 448×448. We do this because we want an odd number of locations in our feature map so there is a single center cell. Objects, especially large objects, tend to occupy the center of the image so it’s good to have a single location right at the center to predict these objects instead of four locations that are all nearby. YOLO’s convolutional layers downsample the image by a factor of 32 so by using an input image of 416 we get an output feature map of 13 × 13.

我们从YOLO中去除了全连接层，使用锚框来预测边界框。首先我们去除了一个pooling层，使网络的卷积层输出分辨率更高。我们还收缩了网络，使其输入为416×416，而不是448×448。我们这样做，因为我们希望特征图中位置数量为奇数，所以有唯一的中心单元。目标，尤其是大目标，通常会占据图像中央，所以有唯一的中央单元有助于预测这些目标。YOLO的卷积层将输入图像进行了32倍下采样，所以416×416的输入图像，输出的特征图为13×13。

When we move to anchor boxes we also decouple the class prediction mechanism from the spatial location and instead predict class and objectness for every anchor box. Following YOLO, the objectness prediction still predicts the IOU of the ground truth and the proposed box and the class predictions predict the conditional probability of that class given that there is an object.

当我们开始使用锚框后，我们也改变了类别预测的机制，从空间位置预测，改为对每个锚框预测类别和objectness。与YOLO一样的是，objectness预测仍然预测真值框和候选框的IOU，类别预测是在假设这里有一个目标的前提下，预测类别的条件概率。

Using anchor boxes we get a small decrease in accuracy. YOLO only predicts 98 boxes per image but with anchor boxes our model predicts more than a thousand. Without anchor boxes our intermediate model gets 69.5 mAP with a recall of 81%. With anchor boxes our model gets 69.2 mAP with a recall of 88%. Even though the mAP decreases, the increase in recall means that our model has more room to improve.

使用锚框后，准确率略有下降。YOLO在每个图像上只预测98个框，但使用锚框后，我们的模型预测超过了1000个框。没有使用锚框的时候，我们的模型mAP为69.5%，召回率81%。使用锚框后，模型mAP下降到69.2%，召回率88%。即使mAP下降了，但召回率上升了，这意为着我们的模型还有更多改进的空间。

**Dimension Clusters**. We encounter two issues with anchor boxes when using them with YOLO. The first is that the box dimensions are hand picked. The network can learn to adjust the boxes appropriately but if we pick better priors for the network to start with we can make it easier for the network to learn to predict good detections.

**维度聚类**。当在YOLO中使用锚框时，我们遇到了两个问题。第一是框的维度是手工选择的。网络可以学习近似调整框，但如果我们选择了更好的先验锚框，网络开始的起点就更好，网络就可以更容易的学习预测更好的结果。

Instead of choosing priors by hand, we run k-means clustering on the training set bounding boxes to automatically find good priors. If we use standard k-means with Euclidean distance larger boxes generate more error than smaller boxes. However, what we really want are priors that lead to good IOU scores, which is independent of the size of the box. Thus for our distance metric we use:

我们没有手工选择先验锚框，而是在训练集的边界框上运行k均值聚类，自动找到好的先验框。如果我们使用标准的欧几里得距离的k均值，更大的框会比小框产生更多错误。但是，我们真正需要的是能够生成好的IOU分数的先验，这与框的大小是无关的。所以我们使用的距离度量为：

d(box, centroid) = 1 − IOU(box, centroid)

We run k-means for various values of k and plot the average IOU with closest centroid, see Figure 2. We choose k = 5 as a good tradeoff between model complexity and high recall. The cluster centroids are significantly different than hand-picked anchor boxes. There are fewer short, wide boxes and more tall, thin boxes.

我们试验k均值的各种k值，画出了距离最近的重心的平均IOU，如图2所示。我们选择k=5，这是模型复杂度与召回率的很好的折中值。聚类中心与手工选择的锚框明显不同。矮宽的框更少，高瘦的框更多。

Figure 2: Clustering box dimensions on VOC and COCO. We run k-means clustering on the dimensions of bounding boxes to get good priors for our model. The left image shows the average IOU we get with various choices for k. We find that k = 5 gives a good tradeoff for recall vs. complexity of the model. The right image shows the relative centroids for VOC and COCO. Both sets of priors favor thinner, taller boxes while COCO has greater variation in size than VOC.

图2：VOC和COCO上的锚框维度聚类。我们对边界框的维度进行k均值聚类，以为模型得到好的先验框。左图是对于各种k值，得到的各种平均IOU。我们发现k=5时，模型的复杂度和召回率之间可以较好的均衡。右图是VOC和COCO的相对重心，两个数据集都得到了较高较瘦的锚框，而COCO的锚框的大小变化比VOC更多。

We compare the average IOU to closest prior of our clustering strategy and the hand-picked anchor boxes in Table 1. At only 5 priors the centroids perform similarly to 9 anchor boxes with an average IOU of 61.0 compared to 60.9. If we use 9 centroids we see a much higher average IOU. This indicates that using k-means to generate our bounding box starts the model off with a better representation and makes the task easier to learn.

我们比较了聚类策略选择出来的最接近先验的平均IOU和手工选择的锚框的平均IOU，如表1所示。在只有5个先验重心的时候，就与9个锚框的平均IOU很接近了，分别是60.9%和61.0%。如果我们使用9个重心，得到的平均IOU要高的多。这意味着使用k均值生成的边界框起点较好，使任务较容易学习。

Table 1: Average IOU of boxes to closest priors on VOC 2007. The average IOU of objects on VOC 2007 to their closest, unmodified prior using different generation methods. Clustering gives much better results than using hand-picked priors.

表1：VOC2007中的最接近先验的平均IOU。VOC2007中的目标与其最接近的，未修正的先验锚框的平均IOU值，锚框是由不同方法生成的。聚类得到的结果比使用手工选取的先验得到更好的结果。

Box Generation | # | Avg IOU
--- | --- | ---
Cluster SSE | 5 | 58.7
Cluster IOU | 5 | 61.0
Anchor Boxes [15] | 9 | 60.9
Cluster IOU | 9 | 67.2

**Direct location prediction**. When using anchor boxes with YOLO we encounter a second issue: model instability, especially during early iterations. Most of the instability comes from predicting the (x, y) locations for the box. In region proposal networks the network predicts values $t_x$ and $t_y$ and the (x, y) center coordinates are calculated as:

**直接位置预测**。当在YOLO中使用锚框时，我们遇到了第二个问题：模型稳定性，尤其是在早期的迭代中。多数不稳定性来自预测框的(x,y)位置。在RPN中，网络预测$t_x$和$t_y$值，(x,y)中心坐标计算如下：

$$x = (t_x ∗ w_a ) − x_a$$
$$y = (t_y ∗ h_a ) − y_a$$

For example, a prediction of $t_x$ = 1 would shift the box to the right by the width of the anchor box, a prediction of $t_x$ = −1 would shift it to the left by the same amount.

例如，$t_x$ = 1会使框向右移动锚框的宽度那么多像素，而$t_x$ = -1会使框向左平移相同的距离。

This formulation is unconstrained so any anchor box can end up at any point in the image, regardless of what location predicted the box. With random initialization the model takes a long time to stabilize to predicting sensible offsets.

这个公式是没有约束的，所以任何锚框都可以在图像的任意位置上，不论在什么位置上预测的这个框。在随机初始化的情况下，模型要花很长时间才能稳定的预测有意义的偏移。

Instead of predicting offsets we follow the approach of YOLO and predict location coordinates relative to the location of the grid cell. This bounds the ground truth to fall between 0 and 1. We use a logistic activation to constrain the network’s predictions to fall in this range.

我们没有预测偏移，而是采用YOLO的方法，预测相对于格子单元位置的坐标。这将真值限制在0和1之间。我们使用logistic激活来限制网络的预测，使预测落在这个范围内。

The network predicts 5 bounding boxes at each cell in the output feature map. The network predicts 5 coordinates for each bounding box, $t_x$, $t_y$, $t_w$, $t_h$, and $t_o$. If the cell is offset from the top left corner of the image by ($c_x$, $c_y$) and the bounding box prior has width and height $p_w$, $p_h$, then the predictions correspond to:

网络在输出特征图的每个单元中预测5个边界框。网络对于每个边界框预测5个坐标值，$t_x$, $t_y$, $t_w$, $t_h$和$t_o$。如果单元是处于图像左上角偏移($c_x$, $c_y$)的位置，先验边界框宽度和高度为$p_w$, $p_h$，那么预测对应着：

$$b_x = σ(t_x) + c_x$$
$$b_y = σ(t_y) + c_y$$
$$b_w = p_w e^{t_w}$$
$$b_h = p_h e^{t_h}$$
$$Pr(object) ∗ IOU(b, object) = σ(t_o)$$

Since we constrain the location prediction the parametrization is easier to learn, making the network more stable. Using dimension clusters along with directly predicting the bounding box center location improves YOLO by almost 5% over the version with anchor boxes.

由于我们限制了位置预测，参数的学习相对简单，网络更加稳定。使用维度聚类和直接预测边界框中心位置，与使用锚框的版本相比，YOLO性能的改进有5%的样子。

Figure 3: Bounding boxes with dimension priors and location prediction. We predict the width and height of the box as offsets from cluster centroids. We predict the center coordinates of the box relative to the location of filter application using a sigmoid function.

图3：有先验维度的边界框和位置预测。我们将边界框的宽度和高度预测为聚类重心的偏移。我们使用sigmoid函数预测框的中心坐标点相对于滤波器的位置。

**Fine-Grained Features**.This modified YOLO predicts detections on a 13 × 13 feature map. While this is sufficient for large objects, it may benefit from finer grained features for localizing smaller objects. Faster R-CNN and SSD both run their proposal networks at various feature maps in the network to get a range of resolutions. We take a different approach, simply adding a passthrough layer that brings features from an earlier layer at 26 × 26 resolution.

**细粒度特征**。这种修正YOLO在13×13大小的特征图上预测检测结果。虽然这对于大目标是足够的了，但是更小的目标的定位可能需要更细粒度的特征。Faster R-CNN和SSD的候选网络都是在多个特征层上进行的，覆盖了一定的分辨率范围。我们采取了不同的方法，只增加了一个传递层，利用较靠前的分辨率26×26的层的特征。

The passthrough layer concatenates the higher resolution features with the low resolution features by stacking adjacent features into different channels instead of spatial locations, similar to the identity mappings in ResNet. This turns the 26 × 26 × 512 feature map into a 13 × 13 × 2048 feature map, which can be concatenated with the original features. Our detector runs on top of this expanded feature map so that it has access to fine grained features. This gives a modest 1% performance increase.

这个传递层将较高分辨率的特征与较低分辨率的特征拼接在一起，具体是将邻近的特征堆积为不同的通道，而不是按照空间位置堆积，与ResNet中的恒等映射类似。这将26×26×512的特征图转化为13×13×2048维的特征图，这就可以与原始的特征拼接在一起了。我们的检测器在这个扩展的特征图上运行，这样就可以利用细粒度的特征。这大约可以提升1%的性能。

**Multi-Scale Training**. The original YOLO uses an input resolution of 448 × 448. With the addition of anchor boxes we changed the resolution to 416×416. However, since our model only uses convolutional and pooling layers it can be resized on the fly. We want YOLOv2 to be robust to running on images of different sizes so we train this into the model.

**多尺度训练**。原版YOLO使用的输入分辨率为448×448。使用了锚框之后，我们将分辨率改为416×416。但是，既然我们的模型只使用卷积层和pooling层，所以可以在运行的时候改变大小。我们希望YOLOv2能够在不同分辨率的图像上都可以稳健的运行，所以我们将这个纳入到模型中。

Instead of fixing the input image size we change the network every few iterations. Every 10 batches our network randomly chooses a new image dimension size. Since our model downsamples by a factor of 32, we pull from the following multiples of 32: {320, 352, ..., 608}. Thus the smallest option is 320 × 320 and the largest is 608 × 608. We resize the network to that dimension and continue training.

我们没有固定输入图像的大小，而是每隔几次迭代就改变网络。每10个批次，我们的网络随机选择一种新的图像维度大小。既然我们的网络的下采样倍率为32，我们就从以下32的倍数中选择：{320,352,...,608}。所以最小的选择是320×320，最大的是608×608。我们改变网络适应这个大小，然后继续训练。

This regime forces the network to learn to predict well across a variety of input dimensions. This means the same network can predict detections at different resolutions. The network runs faster at smaller sizes so YOLOv2 offers an easy tradeoff between speed and accuracy.

这种方法迫使网络学习以适应多种不同的输入尺寸。这意味着同样的网络可以预测不同分辨率的检测。这个网络在输入较小时运行速度更快，所以YOLOv2在速度和准确率的折中比较好。

At low resolutions YOLOv2 operates as a cheap, fairly accurate detector. At 288 × 288 it runs at more than 90 FPS with mAP almost as good as Fast R-CNN. This makes it ideal for smaller GPUs, high framerate video, or multiple video streams.

在低分辨率时YOLOv2是一个廉价但还算准确的检测器。分辨率为288×288时，运行速度超过90FPS，mAP几乎与Fast R-CNN一样好。这与小型GPU、高帧率视频或多个视频流的情况切合的非常好。

At high resolution YOLOv2 is a state-of-the-art detector with 78.6 mAP on VOC 2007 while still operating above real-time speeds. See Table 3 for a comparison of YOLOv2 with other frameworks on VOC 2007.

在高分辨率时，YOLOv2是目前最好的检测器，在VOC2007上mAP达到78.6%，运行速度仍然超过实时。表3是YOLOv2与其他框架在VOC 2007上的比较。

**Further Experiments**. We train YOLOv2 for detection on VOC 2012. Table 4 shows the comparative performance of YOLOv2 versus other state-of-the-art detection systems. YOLOv2 achieves 73.4 mAP while running far faster than competing methods. We also train on COCO and compare to other methods in Table 5. On the VOC metric (IOU = .5) YOLOv2 gets 44.0 mAP, comparable to SSD and Faster R-CNN.

**更多实验**。我们在VOC 2012上训练YOLOv2进行检测。表4给出了YOLOv2与其他目前最好的检测系统的性能对比。YOLOv2的成绩为73.4%mAP时，运行速度远远高于其他方法。我们还在COCO上进行训练，与其他方法进行了对比，如表5所示。在VOC度量(IOU=0.5)的情况下，YOLOv2得到了44.0的mAP，与SSD和Faster R-CNN差不多。

## 3. Faster

We want detection to be accurate but we also want it to be fast. Most applications for detection, like robotics or self-driving cars, rely on low latency predictions. In order to maximize performance we design YOLOv2 to be fast from the ground up.

我们希望检测结果准确，也希望检测速度快。多数检测的应用，如机器人或自动驾驶，都依靠低延迟的预测。为使性能最优化，我们在底层设计上就使YOLOv2尽可能的快。

Most detection frameworks rely on VGG-16 as the base feature extractor [17]. VGG-16 is a powerful, accurate classification network but it is needlessly complex. The convolutional layers of VGG-16 require 30.69 billion floating point operations for a single pass over a single image at 224 × 224 resolution.

多数检测框架使用VGG16作为基础的特征提取器[17]。VGG16是一个强大准确的分类网络，但特别复杂。VGG16的卷积层在输入大小为224×224时，每幅图像每次计算都需要30.69billion浮点数计算。

The YOLO framework uses a custom network based on the Googlenet architecture [19]. This network is faster than VGG-16, only using 8.52 billion operations for a forward pass. However, it’s accuracy is slightly worse than VGG-16. For single-crop, top-5 accuracy at 224 × 224, YOLO’s custom model gets 88.0% ImageNet compared to 90.0% for VGG-16.

YOLO框架使用基于GoogLeNet架构定制的网络[19]。这种网络比VGG16快，一次前向过程只有8.52billion次操作。但是，其精度比VGG-16略差。对于224×224大小的剪切图像，YOLO定制模型的top-5准确率在ImageNet上达到88.0%，VGG-16可以达到90.0%。

**Darknet-19**. We propose a new classification model to be used as the base of YOLOv2. Our model builds off of prior work on network design as well as common knowledge in the field. Similar to the VGG models we use mostly 3 × 3 filters and double the number of channels after every pooling step [17]. Following the work on Network in Network (NIN) we use global average pooling to make predictions as well as 1 × 1 filters to compress the feature representation between 3 × 3 convolutions [9]. We use batch normalization to stabilize training, speed up convergence, and regularize the model [7].

**Darknet-19**。我们提出一种新的分类模型，可以作为YOLOv2的基干。我们的模型利用了前人在网络设计上的经验，也利用了很多常识。与VGG模型类似，我们主要使用3×3大小的滤波器，在每次pooling之后，通道数量加倍[17]。遵循Network in Network的经验，我们使用全局平均pooling来进行预测，也使用1×1滤波器来压缩3×3卷积之间的特征表示[9]。我们使用批归一化来稳定训练，加速收敛以及正则化模型[7]。

Our final model, called Darknet-19, has 19 convolutional layers and 5 maxpooling layers. For a full description see Table 6. Darknet-19 only requires 5.58 billion operations to process an image yet achieves 72.9% top-1 accuracy and 91.2% top-5 accuracy on ImageNet.

我们的最终模型，称为Darknet-19，有19个卷积层，5个maxpooling层。完整的描述见表6。Darknet-19只需要5.58 billion运算来处理一幅图像，但是在ImageNet上得到了72.9%的top-1准确度和91.2%的top-5准确度。


**Training for classification**. We train the network on the standard ImageNet 1000 class classification dataset for 160 epochs using stochastic gradient descent with a starting learning rate of 0.1, polynomial rate decay with a power of 4, weight decay of 0.0005 and momentum of 0.9 using the Darknet neural network framework [13]. During training we use standard data augmentation tricks including random crops, rotations, and hue, saturation, and exposure shifts.

**训练分类**。我们在标准ImageNet 1000类分类数据集上训练网络，经过160轮随机梯度下降法训练，初始学习速率为0.1，多项式速率衰减级数为4，权重衰减率0.0005，动量0.9，使用的是Darknet神经网络框架[13]。在训练时，我们使用标准数据扩充技巧，包括随机裁减、旋转及hue, saturation, exposure变化。

As discussed above, after our initial training on images at 224 × 224 we fine tune our network at a larger size, 448. For this fine tuning we train with the above parameters but for only 10 epochs and starting at a learning rate of $10_{−3}$. At this higher resolution our network achieves a top-1 accuracy of 76.5% and a top-5 accuracy of 93.3%.

如上讨论，在224×224大小的图像进行过训练后，我们在更大的尺寸(448×448)上精调网络。对于精调，我们用上述参数进行训练，但只训练10轮，学习速率为0.001。在这个较高分辨率上，我们的网络的top-1准确率为76.5%，top-5准确度为93.3%。

**Training for detection**. We modify this network for detection by removing the last convolutional layer and instead adding on three 3 × 3 convolutional layers with 1024 filters each followed by a final 1 × 1 convolutional layer with the number of outputs we need for detection. For VOC we predict 5 boxes with 5 coordinates each and 20 classes per box so 125 filters. We also add a passthrough layer from the final 3 × 3 × 512 layer to the second to last convolutional layer so that our model can use fine grain features.

**训练检测**。我们为检测修改网络，去掉最后一个卷积层，而增加3个大小3×3、1024个滤波器的卷积层，随后是1×1卷积层，输出数量是检测所需要的数量。对于VOC，我们预测5个框，每个框5个坐标，以及每个框20个类，共125个滤波器。我们还从最后的3×3×512层增加了一个传递层，到倒数第二个卷积层，这样我们的模型可以使用细粒度特征。

We train the network for 160 epochs with a starting learning rate of $10_{−3}$, dividing it by 10 at 60 and 90 epochs. We use a weight decay of 0.0005 and momentum of 0.9. We use a similar data augmentation to YOLO and SSD with random crops, color shifting, etc. We use the same training strategy on COCO and VOC.

我们将网络训练了160轮，初始学习速率为0.001，在第60轮和90轮时学习率除以10。我们使用的权重衰减为0.0005，动量0.9。我们使用与YOLO和SSD类似的数据扩充方法，即随机裁减、色彩变换等。我们在VOC和COCO上使用相同的训练策略。

## 4. Stronger

We propose a mechanism for jointly training on classification and detection data. Our method uses images labelled for detection to learn detection-specific information like bounding box coordinate prediction and objectness as well as how to classify common objects. It uses images with only class labels to expand the number of categories it can detect.

我们提出了一种在分类数据和检测数据上联合训练的机制。我们的方法使用为检测标记的图像来学习检测特有的信息，如边界框坐标预测和objectness，以及怎样分类常见目标。方法使用只有类别标签的图像来扩展可以检测的类别。

During training we mix images from both detection and classification datasets. When our network sees an image labelled for detection we can backpropagate based on the full YOLOv2 loss function. When it sees a classification image we only backpropagate loss from the classification-specific parts of the architecture.

在训练过程中，我们混合检测数据集和分类数据集的图像。当我们的网络遇到检测数据集的图像时，我们可以用完整的YOLOv2损失函数进行反向传播。当遇到分类数据集图像时，我们只将损失函数从分类特有的部分进行反向传播。

This approach presents a few challenges. Detection datasets have only common objects and general labels, like “dog” or “boat”. Classification datasets have a much wider and deeper range of labels. ImageNet has more than a hundred breeds of dog, including “Norfolk terrier”, “Yorkshire terrier”, and “Bedlington terrier”. If we want to train on both datasets we need a coherent way to merge these labels.

这种方法提出了一些挑战。检测数据集只有一些常见目标和一般的标签，如“狗”和“船”。分类数据集的标签种类更广更深入。ImageNet包括了一百多种狗，包括“Norfolk terrier”, “Yorkshire terrier”, and “Bedlington terrier”。如果我们想要在所有数据集上进行训练，我们需要一种一致的合并这些标签的方式。

Most approaches to classification use a softmax layer across all the possible categories to compute the final probability distribution. Using a softmax assumes the classes are mutually exclusive. This presents problems for combining datasets, for example you would not want to combine ImageNet and COCO using this model because the classes “Norfolk terrier” and “dog” are not mutually exclusive.

多数分类方法在所有类别中使用softmax层来计算最终的概率分布。使用softmax意味着假设这些类别互相是排斥的。这为合并数据集造成了困难，比如不能在这个模型里合并COCO和ImageNet，因为类别“Norfolk terrier”和“狗”是互斥的。

We could instead use a multi-label model to combine the datasets which does not assume mutual exclusion. This approach ignores all the structure we do know about the data, for example that all of the COCO classes are mutually exclusive.

我们可以使用一种多标签模型来合并数据集，这种模型不能有互相排斥的假设。这种方法忽略了我们对数据已知的所有结构，比如所有COCO的类别是互斥的。

**Hierarchical classification**. ImageNet labels are pulled from WordNet, a language database that structures concepts and how they relate [12]. In WordNet, “Norfolk terrier” and “Yorkshire terrier” are both hyponyms of “terrier” which is a type of “hunting dog”, which is a type of “dog”, which is a “canine”, etc. Most approaches to classification assume a flat structure to the labels however for combining datasets, structure is exactly what we need.

**层次化的分类**。ImageNet标签是从WordNet中拉取出来的，这是一个语言数据库，把诸多概念及其怎样关联形成结构[12]。在WordNet中，“Norfolk terrier”和“Yorkshire terrier”都是"terrier"的下位词，而“terrier”都是“猎狗”的一类，“猎狗”又是“狗”的一类，等等。多数分类方法假设标签是水平结构的，而对于合并数据集来说，结构正是我们所需要的。

WordNet is structured as a directed graph, not a tree, because language is complex. For example a “dog” is both a type of “canine” and a type of “domestic animal” which are both synsets in WordNet. Instead of using the full graph structure, we simplify the problem by building a hierarchical tree from the concepts in ImageNet.

WordNet的结构是一种有向图，而不是树，因为语言是复杂的。比如，狗是一种犬类，也是一种家养动物，这在WordNet中都是同义词集合。我们没有使用完整的图结构，而是简化了问题，从ImageNet中的概念中构建了一个层次化的树。

To build this tree we examine the visual nouns in ImageNet and look at their paths through the WordNet graph to the root node, in this case “physical object”. Many synsets only have one path through the graph so first we add all of those paths to our tree. Then we iteratively examine the concepts we have left and add the paths that grow the tree by as little as possible. So if a concept has two paths to the root and one path would add three edges to our tree and the other would only add one edge, we choose the shorter path.

为构建这个树，我们检查了ImageNet中的视觉名词，以及其在WordNet图中的路径，直到根节点，在本例中是“实体目标”。很多同义词集合在图中都只有一条路径，所以我们先将这种路径加入树中。然后我们迭代检查剩下的概念，将路径加入树中，使树的增长越少越好。所以如果一个概念有两条路径到根节点，一条路径将加入三条边，另一条只增加一条边，那么我们选择更短的那条路径。

The final result is WordTree, a hierarchical model of visual concepts. To perform classification with WordTree we predict conditional probabilities at every node for the probability of each hyponym of that synset given that synset. For example, at the “terrier” node we predict:

最终的结果是WordTree，一个可视化概念的层次化模型。为使用WordTree进行分类，我们在每个节点预测条件概率，即在给定同义词集合的情况下每个同义词集合的下位词的概率。比如，在terrier节点，我们预测：

Pr(Norfolk terrier|terrier)

Pr(Yorkshire terrier|terrier)

Pr(Bedlington terrier|terrier)

...

If we want to compute the absolute probability for a particular node we simply follow the path through the tree to the root node and multiply to conditional probabilities. So if we want to know if a picture is of a Norfolk terrier we compute:

如果我们希望计算任一节点的绝对概率，我们只需要顺着树中的路径直到根节点，乘以条件概率。所以如果我们想知道一幅图像是否是Norfolk terrier，我们计算：

Pr(Norfolk terrier) = Pr(Norfolk terrier|terrier) ∗ Pr(terrier|hunting dog) ∗ . . . ∗ Pr(mammal|animal) ∗ Pr(animal|physical object)

For classification purposes we assume that the the image contains an object: Pr(physical object) = 1. 对于分类应用，我们假设图像肯定包含一个目标，即Pr(physical object) = 1。

To validate this approach we train the Darknet-19 model on WordTree built using the 1000 class ImageNet. To build WordTree1k we add in all of the intermediate nodes which expands the label space from 1000 to 1369. During training we propagate ground truth labels up the tree so that if an image is labelled as a “Norfolk terrier” it also gets labelled as a “dog” and a “mammal”, etc. To compute the conditional probabilities our model predicts a vector of 1369 values and we compute the softmax over all sysnsets that are hyponyms of the same concept, see Figure 5.

为验证这个方法，我们用ImageNet的1000类建立了WordTree，在这上面训练Darknet-19模型。为构建WordTree1k我们加入了所有的中间节点，这些中间节点支撑起了1000到1369的标签空间。在训练过程中，我们将真值标签沿着树进行扩散，这样一幅标记为“Norfolk terrier”的狗，就也会标记为“狗”和“哺乳动物”，等等。为计算条件概率，我们的模型预测1369维的向量，计算所有同义词集合（是相同概念的下位词的）上的softmax，如图5所示。

Figure 5: Prediction on ImageNet vs WordTree. Most ImageNet models use one large softmax to predict a probability distribution. Using WordTree we perform multiple softmax operations over co-hyponyms.

图5. 在ImageNet上的预测和在WordTree上的预测。多数ImageNet模型使用一个大的softmax来预测概率分布；使用WordTree我们在同为下位词的概念上进行多重softmax操作。

Using the same training parameters as before, our hierarchical Darknet-19 achieves 71.9% top-1 accuracy and 90.4% top-5 accuracy. Despite adding 369 additional concepts and having our network predict a tree structure our accuracy only drops marginally. Performing classification in this manner also has some benefits. Performance degrades gracefully on new or unknown object categories. For example, if the network sees a picture of a dog but is uncertain what type of dog it is, it will still predict “dog” with high confidence but have lower confidences spread out among the hyponyms.

我们与之前使用相同的训练参数，层次化的Darknet-19得到了71.9%的top-1准确率，和90.4%的top-5准确率。尽管增加了369个额外的概念，网络预测的是一个树形结构，我们的准确率下降的非常少。以这种方式执行分类，也有一些好处。在新的或未知的目标类别上，性能下降的非常平滑。比如，如果网络看到了一幅狗的图像，但不确定是什么样的狗，它仍会以很高的置信度预测为“狗”，在其他下位词上则置信度较低。

This formulation also works for detection. Now, instead of assuming every image has an object, we use YOLOv2’s objectness predictor to give us the value of Pr(physical object). The detector predicts a bounding box and the tree of probabilities. We traverse the tree down, taking the highest confidence path at every split until we reach some threshold and we predict that object class.

这种方式也对检测有效。现在，我们不是假设每个图像都包含目标，而是使用YOLOv2的objectness预测器来给定概率Pr(physical object)的值。检测器预测一个边界框，以及概率树。我们沿着树穿过，在每次分裂时取最大的置信度路径，直到我们达到某个阈值，然后我们就预测这个目标类别。

**Dataset combination with WordTree**. We can use WordTree to combine multiple datasets together in a sensible fashion. We simply map the categories in the datasets to synsets in the tree. Figure 6 shows an example of using WordTree to combine the labels from ImageNet and COCO. WordNet is extremely diverse so we can use this technique with most datasets.

**使用WordTree数据集合并**。我们可以使用WordTree来合理的合并多个数据集。我们只要将数据集中的类别映射到树中的同义词集合。图6给出了使用WordTree合并ImageNet和COCO标签的例子。WordNet覆盖范围极广，所以我们可以对大多数数据集使用这种方法。

Figure 6: Combining datasets using WordTree hierarchy. Using the WordNet concept graph we build a hierarchical tree of visual concepts. Then we can merge datasets together by mapping the classes in the dataset to synsets in the tree. This is a simplified view of WordTree for illustration purposes.

图6：使用WordTree层次结构合并数据集。使用WordTree概念图，我们构建了一个视觉概念的层次树。我们可以通过将数据集中的类映射到树中的同义词集合中，这样来合并数据集。这是一个WordTree的简化示意图。

**Joint classification and detection**. Now that we can combine datasets using WordTree we can train our joint model on classification and detection. We want to train an extremely large scale detector so we create our combined dataset using the COCO detection dataset and the top 9000 classes from the full ImageNet release. We also need to evaluate our method so we add in any classes from the ImageNet detection challenge that were not already included. The corresponding WordTree for this dataset has 9418 classes. ImageNet is a much larger dataset so we balance the dataset by oversampling COCO so that ImageNet is only larger by a factor of 4:1.

**联合分类和检测**。现在我们可以使用WordTree合并数据集，我们就可以训练模型同时进行分类和检测。我们想训练一个极大规模的检测器，所以我们将COCO检测数据集和全部ImageNet的前9000类合并为我们用的数据集。我们还需要评估我们的方法，所以我们把ImageNet检测数据集加入进来。这个数据集对应的WordTree有9418类。ImageNet是一个大的多的数据集，所以我们对COCO进行过采样来均衡数据集，使ImageNet数据集与COCO数据集比例达到4:1。

Using this dataset we train YOLO9000. We use the base YOLOv2 architecture but only 3 priors instead of 5 to limit the output size. When our network sees a detection image we backpropagate loss as normal. For classification loss, we only backpropagate loss at or above the corresponding level of the label. For example, if the label is “dog” we do assign any error to predictions further down in the tree, “German Shepherd” versus “Golden Retriever”, because we do not have that information.

使用这个数据集，我们训练YOLO9000。我们使用YOLOv2架构作为基础，但只使用5个先验中的3个来限制输出规模。当网络训练输入为一幅检测图像，我们正常将损失函数反向传播。对于分类损失，我们只在标签之上反向传播损失。比如，如果标签为“狗”，我们将树中的任何误差都纳入到预测来，而“German Shepherd” versus “Golden Retriever”这样的则不行，因为我们没有这些信息。

When it sees a classification image we only backpropagate classification loss. To do this we simply find the bounding box that predicts the highest probability for that class and we compute the loss on just its predicted tree. We also assume that the predicted box overlaps what would be the ground truth label by at least .3 IOU and we backpropagate objectness loss based on this assumption.

当网络遇到一幅分类图像时，我们只反向传播分类损失函数。我们只找到预测那个类的最高概率的边界框，计算其损失。我们还假设，预测的框与真值框的IOU至少有0.3，基于这个假设，我们反向传播objectness损失。

Using this joint training, YOLO9000 learns to find objects in images using the detection data in COCO and it learns to classify a wide variety of these objects using data from ImageNet.

使用这种联合训练，YOLO9000从COCO中的检测数据来学习寻找目标，从ImageNet数据集中的数据来学习分类目标。

We evaluate YOLO9000 on the ImageNet detection task. The detection task for ImageNet shares on 44 object categories with COCO which means that YOLO9000 has only seen classification data for the majority of the test images, not detection data. YOLO9000 gets 19.7 mAP overall with 16.0 mAP on the disjoint 156 object classes that it has never seen any labelled detection data for. This mAP is higher than results achieved by DPM but YOLO9000 is trained on different datasets with only partial supervision [4]. It also is simultaneously detecting 9000 other object categories, all in real-time.

我们在ImageNet检测任务中评估YOLO9000模型。ImageNet检测任务与COCO数据集共享44类目标，这意味着YOLO9000对于大多数测试数据来说，只在分类数据中进行过训练，没有在检测数据中进行过训练。YOLO9000得到的总体表现为19.7% mAP，在那156类中的成绩为16.0% mAP，这156类数据网络没有其标注过的数据。得到的mAP比DPM高，但是要考虑到YOLO9000是在不同的数据集上训练的，只有部分的supervision[4]。而同时网络还可以检测其他9000种数据，都是实时的。

When we analyze YOLO9000’s performance on ImageNet we see it learns new species of animals well but struggles with learning categories like clothing and equipment. New animals are easier to learn because the objectness predictions generalize well from the animals in COCO. Conversely, COCO does not have bounding box label for any type of clothing, only for person, so YOLO9000 struggles to model categories like “sunglasses” or “swimming trunks”.

当我们分析YOLO9000在ImageNet上的性能时，我们看到网络很好的学习了新的种类，但有些类别如衣服和装备却学习不好。新的动物更容易学习，因为其objectness预测从COCO中泛化的很好。相反的，COCO没有任何衣服类别的标注框，只有人这个类别的，所以YOLO9000在对太阳镜或泳装这样的类别建模比较困难。

## 5. Conclusion 结论

We introduce YOLOv2 and YOLO9000, real-time detection systems. YOLOv2 is state-of-the-art and faster than other detection systems across a variety of detection datasets. Furthermore, it can be run at a variety of image sizes to provide a smooth tradeoff between speed and accuracy.

我们提出了YOLOv2和YOLO9000两种实时检测系统。YOLOv2是目前最好的系统，在很多数据集上都比其他数据集检测速度快的多。而且，系统可以在很多图像尺寸上运行，在速度和准确度上的平衡很好。

YOLO9000 is a real-time framework for detection more than 9000 object categories by jointly optimizing detection and classification. We use WordTree to combine data from various sources and our joint optimization technique to train simultaneously on ImageNet and COCO. YOLO9000 is a strong step towards closing the dataset size gap between detection and classification.

YOLO9000是一种实时检测框架，通过联合对检测和分类进行优化，可以检测超过9000类目标。我们使用WordTree来合并不同数据集的数据，使用我们的联合优化技术在ImageNet和COCO上同时训练。YOLO9000在填补检测数据集和分类数据集之间的空白上作出了很好的贡献。

Many of our techniques generalize outside of object detection. Our WordTree representation of ImageNet offers a richer, more detailed output space for image classification. Dataset combination using hierarchical classification would be useful in the classification and segmentation domains. Training techniques like multi-scale training could provide benefit across a variety of visual tasks.

我们的很多技术都可以泛化到目标检测之外。我们对ImageNet的WordTree表示，是图像分类中的一个更丰富细节更多的输出空间。使用层次化分类的数据集合并在分类和分割领域中都会有作用。多尺度训练这样的训练技巧，可以在很多视觉任务应用。

For future work we hope to use similar techniques for weakly supervised image segmentation. We also plan to improve our detection results using more powerful matching strategies for assigning weak labels to classification data during training. Computer vision is blessed with an enormous amount of labelled data. We will continue looking for ways to bring different sources and structures of data together to make stronger models of the visual world.

对于未来的工作，我们希望使用类似的技术进行弱监督图像分割。我们还计划改进检测结果，通过使用更强的匹配策略，在训练时为分类数据指定弱标签。计算机视觉标注数据丰富，我们会继续合并不同结构的数据，训练更强大的模型。
# Models Genesis: Generic Autodidactic Models for 3D Medical Image Analysis

Zongwei Zhou et. al. Arizona State University, Mayo Clinic

## 0. Abstract

Transfer learning from natural image to medical image has established as one of the most practical paradigms in deep learning for medical image analysis. However, to fit this paradigm, 3D imaging tasks in the most prominent imaging modalities (e.g., CT and MRI) have to be reformulated and solved in 2D, losing rich 3D anatomical information and inevitably compromising the performance. To overcome this limitation, we have built a set of models, called Generic Autodidactic Models, nicknamed Models Genesis, because they are created ex nihilo (with no manual labeling), self-taught (learned by self-supervision), and generic (served as source models for generating application-specific target models). Our extensive experiments demonstrate that our Models Genesis significantly outperform learning from scratch in all five target 3D applications covering both segmentation and classification. More importantly, learning a model from scratch simply in 3D may not necessarily yield performance better than transfer learning from ImageNet in 2D, but our Models Genesis consistently top any 2D approaches including fine-tuning the models pre-trained from ImageNet as well as fine-tuning the 2D versions of our Models Genesis, confirming the importance of 3D anatomical information and significance of our Models Genesis for 3D medical imaging. This performance is attributed to our unified self-supervised learning framework, built on a simple yet powerful observation: the sophisticated yet recurrent anatomy in medical images can serve as strong supervision signals for deep models to learn common anatomical representation automatically via self-supervision. As open science, all pre-trained Models Genesis are available at https://github.com/MrGiovanni/ModelsGenesis.

从自然图像到医学图像的迁移学习，是深度学习在医学图像分析中最常见的实践范式之一。但是，为适应这种范式，在绝大多数成像模态中（如CT和MRI），3D的成像任务必须重新表述并在2D中解决，损失了丰富的3D解剖结构信息，并不可避免的对性能有损失。为克服这种限制，我们构建了一系列模型，称为通用自学习模型，昵称为起源模型(Model Genesis)，因为他们是从无中生有的（不需要手动标注），自学习的（通过自监督学习的），并且是通用的（作为生成具体应用的目标模型的源模型）。我们广泛的实验证明了，我们的Model Genesis，在所有五种目标3D应用中，都显著超出了从头学习的效果，包括分割和分类。更重要的是，仅仅在3D中从头学习模型，很可能不一定得到，比从2D ImageNet中迁移学习的更好结果，但我们的Model Genesis会一直超过任意的2D方法，包括精调ImageNet预训练模型，以及精调2D版本的我们的Model Genesis，确认了3D解剖结构信息的重要性，和Model Genesis对于3D医学成像的重要性。这种性能归功于我们统一的自监督学习框架，构建在一种简单却很强的观察之上：医学图像中复杂但周期性的解剖结构，本身就可以对深度模型是一种很强的监督信号，可以自动通过自监督，学习常见的解剖结构表示。作为开放科学，所有的预训练的Model Genesis都已开源。

## 1 Introduction

Given the marked differences between natural images and medical images, we hypothesize that transfer learning can yield more powerful (application-specific) target models if the source models are built directly from medical images. To test this hypothesis, we have chosen chest imaging because the chest contains several critical organs, which are prone to a number of diseases that result in substantial morbidity and mortality and thus are associated with significant health-care costs. In this research, we focus on Chest CT, because of its prominent role in diagnosing lung diseases, and our research community has accumulated several Chest CT image databases, for instance, LIDC-IDRI and NLST, containing a large number of Chest CT images. Therefore, we seek to answer the following question: Can we utilize the large number of available Chest CT images without systematic annotation to train source models that can yield high-performance target models via transfer learning?

自然图像和医学图像之间有很大的差异，我们推测，如果源模型是直接从医学图像中构建得到的，那么迁移学习会得到更强力的（应用相关的）目标模型。为测试这个假设，我们选择了胸部成像，因为胸部包含几种关键的器官，很容易患几种疾病，这些疾病会导致很高的发病率和死亡率，因此与相当一部分健康开销相关。在本研究中，我们关注的是胸部CT，因为其在诊断肺部疾病时有主要的作用，我们的研究团队积累了几个胸部CT的图像数据集，比如，LIDC-IDRI和NLST，包含大量胸部CT图像。因此，我们试图回答以下问题：我们是否可以利用大量可用的胸部CT图像，而不需要系统的标注，来训练源模型，再通过迁移学习，来得到高性能的目标模型呢？

To answer this question, we have developed a framework that trains generic, source models for 3D imaging. We call the models trained with our framework Generic Autodidactic Models, nicknamed Models Genesis, and refer to the model trained using Chest CT scans as Genesis Chest CT. As ablation studies, we have also trained a downgraded 2D version using 2D Chest CT slices, called Genesis Chest CT 2D. To demonstrate the effectiveness of Models Genesis in 2D applications, we have trained a 2D model based on ChestX-ray8, named as Genesis Chest X-ray.

为回答这个问题，我们提出了一个框架，训练用于3D成像的通用的源模型。我们称用我们的框架训练出来的模型为通用自学习模型，昵称为Model Genesis，称使用胸部CT scans训练的模型为胸部CT Genesis。在分离研究中，我们还训练了一个降质的2D版，使用的是2D的胸CT切片，称为Genesis胸部CT 2D。为证明Model Genesis在2D应用中的有效性，我们基于ChestX-ray8训练了一个2D模型，命名为Genesis Chest X-ray。

Our extensive experiments detailed in Sec. 3 demonstrate that Models Genesis, including Genesis Chest CT, Genesis Chest CT 2D, and Genesis Chest X-ray, significantly outperform learning from scratch in all seven target tasks (see Table 1). As revealed in Table 4, learning from scratch simply in 3D may not necessarily yield performance better than fine-tuning state-of-the-art ImageNet models, but our Genesis Chest CT consistently top any 2D approaches including fine-tuning ImageNet models as well as fine-tuning our Genesis Chest X-ray and Genesis Chest CT 2D, confirming the importance of 3D anatomical information in Chest CT and significance of our self-supervised learning method in 3D medical image analysis.

我们在第3部分中进行了很多试验，说明了Models Genesis，包括Genesis Chest CT, Genesis Chest CT 2D和Genesis Chest X-ray，在所有七种目标任务中，明显超过了从头学习的结果（见表1）。在表4中可以看到，仅仅在3D中从头学习，并不一定会得到从目前最好的ImageNet模型精调的结果的性能，但我们的Genesis Chest CT比其他任何2D方法都要好，包括精调ImageNet模型，以及精调我们的Genesis Chest X-ray和Genesis Chest CT 2D，这确认了，胸部CT 3D解剖结构信息的重要性，和我们的自监督学习方法在3D医学图像分析中的重要性。

This performance is attributable to the following key observation: medical imaging protocols typically focus on particular parts of the body for specific clinical purposes, resulting in images of similar anatomy. The sophisticated yet recurrent anatomy offers consistent patterns for self-supervised learning to discover common representation of a particular body part (the lungs in our case). The fundamental idea behind our unified self-supervised learning method as illustrated in Fig. 1 is to recover anatomical patterns from images transformed via various ways in a unified framework.

这种表现要归功于以下关键的观察：医学成像协议，对特定的临床目的而言，一般聚焦在身体的特定部位，得到类似解剖结构的图像。复杂但周期性的解剖结构，给出了自监督学习的一致性模型，可以发现特定身体部位的常见表示（在我们的例子中，是肺部）。我们统一的自监督学习方法背后的基础思想，如图1所示，是要从图像中得到解剖结构的模式，而这些图像是用各种方法、以一种统一的框架转换过来的。

## 2 Models Genesis

Models Genesis learn from scratch on unlabeled images, with an objective to yield a common visual representation that is generalizable and transferable across diseases, organs, and modalities. In Models Genesis, an encoder-decoder, as shown in Fig. 1, is trained using a series of self-supervised schemes. Once trained, the encoder alone can be fine-tuned for target classification tasks; while the encoder and decoder together can be for target segmentation tasks. For clarity, we formally define a training scheme as the process that transforms patches with any of the transformations, as illustrated in Fig. 1, and trains a model to restore the original patches from the transformed counterparts. In the following, we first explain each of our self-supervised learning schemes with its learning objectives and perspectives, followed by a summary of the four unique properties of our Models Genesis. Along the way, we also contrast Models Genesis with existing approaches to show our innovations and novelties.

Model Genesis从无标签图像中从头学习，其目标是生成一种通用的视觉表示，在疾病、器官和模态之间可以泛化和迁移。在Model Genesis中，使用一系列自监督的方法训练了一个编码器解码器，如图1所示。一旦训练好了以后，编码器可以进一步精调，用于目标的分类任务；而编码器解码器一起，可以用于目标的分割任务。清晰起见，我们正式的定义一个训练方案为，用任意的变换对一个图像块进行处理的过程，如图1所示，并训练一个模型从变换过的部分恢复到原始的图像块。下面，我们首先解释，自监督的学习方案的每个部分，用的是其学习目标和观点进行解释，然后列举了我们的Model Genesis的四种唯一属性的总结。沿着这条路径，我们还将Model Genesis和现有的方法进行了比较，以体现我们的创新点。

Fig. 1: Our unified self-supervised learning framework consolidates four novel transformations: I) non-linear, II) local-shuffling, III) out-painting, and IV) in-painting into a single image restoration task. Specifically, each arbitrarily-size patch X cropped at random location from an unlabeled image can undergo at most three of above transformations, resulting in a transformed patch $\tilde X$ (see I–V). Note that out-painting and in-painting are mutually exclusive. For simplicity and clarity, we illustrate our idea on a 2D CT slice, but our Genesis Chest CT is trained using 3D images directly. A Model Genesis, an encoder-decoder architecture, is trained to learn a common visual representation by restoring the original patch X (as ground truth) from the transformed one $\tilde X$ (as input), aiming to yield high-performance target models.

**Learning appearance via non-linear transformation**. Absolute or relative intensity values in medical images convey important information about the imaged structures and organs. For instance, the Hounsfield Units in CT scans correspond to specific substances of the human body. As such, intensity information can be used as a strong source of pixel-wise supervision. To preserve relative intensity information of anatomies during image transformation, we use Bezier Curve, a smooth and monotonous transformation function, which assigns every pixel a unique value, ensuring a one-to-one mapping. Restoring image patches distorted with non-linear transformation focuses Models Genesis on learning organ appearance (shape and intensity distribution). Fig. 1–I shows examples of the transformed images. Due to limited space, we provide the implementation details in Appendix Sec. B.

**通过非线性变换学习外观**。医学图像中的绝对或相对灰度值，可以传递成像结构和器官的重要信息。比如，CT scans的HU值对应着人体的特定物质。这样，灰度信息可以用作逐像素监督的很强的源。为在图像变换时保护解剖结构的相对灰度信息，我们使用Bezier曲线，一种平滑、单调的变换函数，对每个像素都指定一个唯一值，确保一对一的映射。将使用非线性变换变形过的图像恢复回来，使得Model Genesis聚焦在学习器官外观上（即形状和灰度分布）。图1-I给出了变换过的图像的例子。由于空间有限，我们在附录B中给出实现细节。

**Learning texture via local pixel shuffling**. Given an original patch, local pixel shuffling consists of sampling a random window from the patch followed by shuffling the order of contained pixels resulting in a transformed patch. The size of the local window determines the task difficulty, but we keep it smaller than the model’s receptive field, and also small enough to prevent changing the global content of the image. Note that our method is quite different from PatchShuffling [5], which is a regularization technique to avoid over-fitting. To recover from local pixel shuffling, Models Genesis must memorize local boundaries and texture. Examples of local-shuffling are illustrated in Fig. 1–II. We include the underlying mathematics and implementation details in Appendix Sec. C.

**通过局部像素重组来学习纹理**。给定原始图像块，局部像素重组是，从图像块中对一个随机窗口进行取样，然后对所包含的像素进行重组，得到一个变换过的图像块。局部窗口的大小，确定了任务的难度，但我们保持其比模型的感受野要小，也要足够小，以防止改变图像的全局内容。注意，我们的方法与PatchShuffling[5]是很不一样的，那只是一个正则化技术，防止过拟合。为从局部像素重组中恢复，Model Genesis必须记住局部边缘和纹理。局部种族的例子如图1-II所示。我们在附录C中给出潜在的数学和实现细节。

**Learning context via out-painting and in-painting**. To realize the self-supervised learning via out-painting, we generate an arbitrary number of windows of various sizes and aspect ratios, and superimpose them on top of each other, resulting in a single window of a complex shape. We then assign a random value to all pixels outside the window while retaining the original intensities for the pixels within. As for in-painting, we retain the original intensities outside the window and replace the intensity values of the inner pixels with a constant value. Unlike [6], where in-painting is proposed as a proxy task by restoring only the patch central region, we restore the entire patch in the output. Out-painting compels Models Genesis to learn global geometry and spatial layout of organs via extrapolating, while in-painting requires Models Genesis to appreciate local continuities of organs via interpolating. Examples of out-painting and in-painting are shown in Fig. 1–III and Fig. 1–IV, respectively. More visualizations can be found in Appendix Secs. D—E.

**通过out-painting和in-painting学习上下文**。为通过out-painting实现自监督，我们生成任意数量的各种大小和纵横比的窗口，并将其互相叠放到一起，得到一个复杂形状的窗口。然后我们对窗口外的所有像素指定一个随机值，同时保持内部像素的原始灰度。至于in-painting，我们保持窗口外的原始像素灰度值，同时将内部像素的灰度值替换为一个常数值。与[6]不同，其中in-painting用作代理任务，只恢复中央区域的小块，我们在输出时恢复的是整个图像块。Out-painting迫使Model Genesis通过外插来学习全局几何和器官空间分布，而in-painting需要Model Genesis通过内插来学习器官的局部连续性。Out-painting和in-painting的例子，分别如图1-III和图1-IV。更多的可视化示例可见附录D-E。

Models Genesis have the following four unique properties: Model Genesis有四个如下的唯一性质：

**1) Autodidactic—requiring no manual labeling**. Models Genesis are trained in a self-supervised manner with abundant unlabeled image datasets, demanding zero expert annotation effort. Consequently, Models Genesis are very different from traditional supervised transfer learning from ImageNet [7,9], which offers modest benefit to 3D medical imaging applications as well as that from the pretrained models of NiftyNet, which is ineffective (see Sec. 3 and Appendix Sec. I) due to the small datasets and specific applications (e.g., brain parcellation and organ segmentation) these models are trained for.

**1)自学习 - 不需要手动标注**。Model Genesis是通过自监督的形式进行训练的，有大量的未标注的图像数据集，不需要专家的标注工作。结果是，Model Genesis与传统的有监督的从ImageNet的迁移学习[7,9]非常不一样，那些工作只为3D医学图像应用有些许好处，以及从NiftyNet的预训练模型中，这是效率很低下的（见第3部分和附录第I部分），因为数据集过小，也因为训练的模型是用于特定应用的（如，脑部分割和器官分割）。

**2) Eclectic—learning from multiple perspectives**. Our unified approach trains Models Genesis from multiple perspectives (appearance, texture, context, etc.), leading to more robust models across all target tasks, as evidenced in Table 3, where our unified approach is compared with our individual schemes. This eclectic approach, incorporating multiple tasks into a single image restoration task, empowers Models Genesis to learn more comprehensive representation.

**2) 兼容并包的 - 从各种角度进行学习**。我们的统一方法从多个角度训练Models Genesis（外观，纹理，上下文，等等），这为所有目标任务带来了更稳健的模型，如表3所示，其中我们的统一方法与我们的独立方案进行了比较，这种兼容并包的方法，将多种任务包含进入一个图像恢复任务，让Models Genesis学习到了更综合的表示。

**3) Scalable—eliminating proxy-task-specific heads**. Consolidated into a single image restoration task, our novel self-supervised schemes share the same encoder and decoder during training. Had each task required its own decoder, due to limited memory on GPUs, our framework would have failed to accommodate a large number of self-supervised tasks. By unifying all tasks as a single image restoration task, any favorable transformation can be easily amended into our framework, overcoming the scalability issue associated with multi-task learning [2], where the network heads are subject to the specific proxy tasks.

**3) 可扩展 - 消除了与具体任务特定的头**。我们的自监督方案合并到了一个图像恢复任务中，在训练时共享到了相同的编码器-解码器。如果每个任务需要其自己的解码器，由于在GPUs上使用的内存有限，我们的框架可能不能容纳很多自监督的任务。通过将所有任务统一到一个图像恢复任务中，任何合适的变换都可以修改放入到这个框架中，克服了多任务学习相关的可扩展问题，其中网络头是与具体的代理任务相关的。

**4) Generic—yielding diverse applications**. Models Genesis learn a general-purpose image representation that can be leveraged for a wide range of target tasks. Specifically, Models Genesis can be utilized to initialize the encoder for the target classification tasks and to initialize the encoder-decoder for the target segmentation tasks, while the existing self-supervised approaches are largely focused on providing encoder models only [4]. As shown in Table 2, Models Genesis can be generalized across diseases (e.g., nodule, embolism, tumor), organs (e.g., lung, liver, brain), and modalities (e.g., CT, X-ray, MRI), a generic behavior that sets us apart from all previous works in the literature where the representation is learned via a specific self-supervised task; and thus lack generality. Such specific schemes include predicting the distance and 3D coordinates of two patches randomly sampled from a same brain [8], identifying whether two scans belong to the same person, predicting the level of vertebral bodies [3], and finally the systematic study by Tajbakhsh et al. [10] where individualized self-supervised schemes are studied for a set of target tasks.

**4) 通用 - 产生众多的应用**。Models Genesis学习的是通用目标的图像表示，可以用于很宽范围内的目标任务中。具体来说，Models Genesis可以用于对目标分类任务的编码器进行初始化，对于目标分割任务的编码器解码器进行初始化，而现有的自监督方法主要关注的是只给出编码器本身。如表2所示，Models Genesis可以在各种疾病（如，结节，帅塞，肿瘤），各种器官（如，肺，肝脏，大脑），和各种模态（如，CT，X射线，MRI中泛化使用，之前文献中的工作，其表示都是通过一种特定的自监督的任务中学习到的，而我们的工作则是通用的，与之前的缺少泛化性的工作明显不一样。这样的具体方法包括，预测从一个大脑中随机取样得到的两个图像块之间的距离和3D坐标，识别两个scans是否属于同一个人，预测脊椎体的层次，最后是Tajbakhsh等人的系统的研究，即对一些目标任务研究了各种自监督的方案。

## 3 Experiments and Results

**Experiment protocol**. Our Genesis CT and Genesis X-ray are self-supervised pre-trained from 534 CT scans in LIDC-IDRI and 77,074 X-rays in ChestX-ray8, respectively. The reason that we decided not to use all images in LIDC-IDRI and in ChestX-ray8 for training Models Genesis is to avoid test-image leaks between proxy and target tasks, so that we can confidently use the rest images solely for testing Models Genesis as well as the target models, although Models Genesis are trained from only unlabeled images, involving no annotation shipped with the datasets. We evaluate Models Genesis in seven medical imaging applications including 3D and 2D image classification and segmentation tasks (codified as detailed in Table 1). For 3D applications in CT and MRI, we investigate the capability of both 2D slice-based solutions and 3D volume-based solutions; for 2D applications in X-ray and Ultrasound, we compare Models Genesis with random initialization and fine-tuning from ImageNet. 3D U-Net architecture is used in five 3D applications; U-Net architecture with ResNet-18 encoder is used in seven 2D applications. We utilize the L1-norm distance as the loss function in the image restoration tasks. Performances of target image classification and segmentation tasks are measured by the AUC (Area Under the Curve) and IoU (Intersection over Union), respectively, through at least 10 trials. We report the performance metrics with mean and standard deviation and further present statistical analysis based on the independent two-sample t-test.

**试验协议**。我们的Genesis CT和Genesis X-ray，分别是从LIDC-IDRI的534个CT scans中，和ChestX-ray8的77074张X射线照片中，自监督预训练得到的。我们决定不使用LIDC-IDRI和ChestX-ray8中的所有图像来进行训练，其原因是防止测试图像在代理任务和目标任务之间泄漏，所以我们可以很有信心使用剩下的图像，来测试Models Genesis以及目标模型，虽然Models Genesis都是从无标记的图像中训练的，不涉及到从其他数据集中带来的标注。我们在7个医学成像应用中评估Models Genesis，包括3D和2D图像分类和分割任务（详见表1）。对于在CT和MRI中的3D应用，我们研究了基于2D切片的解决方案和基于3D体的解决方案；对于X射线和超声的2D应用，我们比较了Models Genesis和随机初始化和从ImageNet精调。3D U-Net架构用于5个3D应用中；使用ResNet-18编码器的U-Net架构用在了7个2D应用中。我们在图像恢复任务中利用L1-范数距离作为损失函数。目标图像分类和分割任务的性能，分别是用AUC (Area Under the Curve) 和 IoU (Intersection over Union)度量的，至少经过了10次尝试。我们给出性能度量的平均值和标准差，进一步基于独立双样本t-test给出统计分析。

Table 1: Target tasks.

Code | Object | Modality | Source | Description
--- | --- | --- | --- | ---
NCC | Lung Nodule | CT | LUNA2016 | Lung nodule false positive reduction
NCS | Lung Nodule | CT | LIDC-IDRI | Lung nodule segmentation
ECC | Pulmonary Embolism | CT | PE-CAD | Pulmonary embolism false positive reduction
LCS | Liver | CT | LiTS2017 | Liver segmentation
DXC | Pulmonary Diseases | X-ray | ChestX-ray8 | Eight pulmonary disease classification
IUC | CIMT RoI | Ultrasound | UFL MCAEL | RoI, bulb, and background classification
BMS | Brain Tumor | MRI | BraTS2013 | Brain tumor segmentation

**Models Genesis outperform 3D models trained from scratch**. We evaluate the effectiveness of Genesis Chest CT in five distinct 3D medical target tasks. These target tasks are selected such that they show varying levels of semantic distance to the proxy task, as shown in Table 2, allowing us to investigate the transferability of Genesis Chest CT with respect to the domain distance. Table 2 demonstrates that models fine-tuned from Genesis Chest CT consistently outperform their counterparts trained from scratch. Our statistical analysis show that the performance gain is significant for all the target tasks under study. Specifically, for NCC and NCS where the target and proxy tasks are in the same domain, initialization with Genesis Chest CT achieves 4 and 3 points increase in the AUC and IoU score, respectively, compared with training from scratch. For ECC, the target and proxy tasks are different in both the disease affecting the organ and the dataset itself; yet, Genesis Chest CT achieves a remarkable improvement over training from scratch, increasing the AUC by 8 points. Genesis Chest CT continues to yield significant IoU gain for LCS and BMS even though their domain distances with the proxy task are the widest. To our knowledge, we are the first to investigate cross-domain self-supervised learning in medical imaging. Given the fact that Genesis Chest CT is pre-trained on Check CT only, it is remarkable that our model can generalize to different diseases, organs, datasets, and even modalities.

**Models Genesis超过了从头训练的3D模型**。我们在5个不同的3D医学目标任务中评估Genesis Chest CT的有效性。这些目标任务的选择，体现了与代理任务的语义距离，如表2所示，使我们可以研究Genesis Chest CT对领域距离的可迁移性。表2证明了，从Genesis Chest CT精调得到的模型一直超过了从头训练的对应模型。我们的统计分析表明，性能提升对所有研究的目标任务都是显著的。特别的，对于NCC和NCS，目标任务和代理任务是同一个领域的，用Genesis Chest CT的初始化，与从头训练相比，在AUC和IoU分数上分别可以得到4个点和3个点的性能提升。对于ECC，目标任务和代理任务，在疾病影响的器官，和数据集本身上，都是不一样的；但是，Genesis Chest CT与从头训练相比，有很明显的改进，AUC增加了8个点。Genesis Chest CT继续对LCS和BMS取得了明显的IoU提升，虽然其与代理任务的领域距离是最宽的。据我们所知，我们是第一个在医学成像中研究跨领域的自监督学习的。鉴于Genesis Chest CT是只在Check CT上预训练的，我们的模型对不同的疾病、器官、数据集，甚至模态都可以泛化的很好，这是很让人惊讶的。

Table 2: Fine-tuning models from our Genesis Chest CT (3D) significantly outperforms learning from scratch in the five 3D target tasks (p<0.05). The cells checked by x denote the properties that are different between the proxy and target datasets. Our results show that our Genesis Chest CT generalizes across organs, diseases, datasets, and modalities. Footnotes show state-of-the-art performance for each target task.

Task | Metric | Disease | Organ | Dataset | Modality | Scratch (%) | Genesis (%) | p-value
--- | --- | --- | --- | --- | --- | --- | --- | ---
NCC | AUC | o | o | o | o | 94.25±5.07 | 98.20±0.51 | 0.0180
NCS | IoU | o | o | o | o | 74.05±1.97 | 77.62±0.64 | 1.04e-4
ECC | AUC | x | o | x | o | 79.99±8.06 | 88.04±1.40 | 0.0058
LCS | IoU | x | x | x | o | 74.60±4.57 | 79.52±4.77 | 0.0361
BMS | IoU | x | x | x | x | 90.16±0.41 | 90.60±0.20 | 0.0041

**Models Genesis consistently top any 2D approaches**. A common technique to handle limited data in medical imaging is to reformat 3D data into a 2D image representation followed by fine-tuning pre-trained ImageNet models [7,9]. This approach increases the training examples by an order of magnitude, but it scarifies the 3D context. It is interesting to compare how Genesis Chest CT compares to this de facto standard in 2D. For this purpose, we adopt the trained 2D models from an ImageNet pre-trained model for the tasks of NCC, NCS, and ECC. The 2D representation is obtained by extracting axial slices from volumetric datasets. Table 4 compares the results for 2D and 3D models. Note that the results for 3D models are identical to those reported in Table 2. As evidenced by our statistical analyses, the 3D models trained from Genesis Chest CT significantly outperform the 2D models trained from ImageNet, achieving higher average performance and lower standard deviation (see Table 4 and Appendix Sec. H). However, the same conclusion does not apply to the models trained from scratch—3D scratch models outperform 2D scratch models in only two out of the three target tasks and also exhibit undesirably larger standard deviation. We attribute the mixed results of 3D scratch models to the larger number of model parameters and limited sample size in the target tasks, which together impede the full utilization of 3D context. In fact, the undesirable performance of the 3D scratch models highlights the effectiveness of Genesis Chest CT, which unlocks the power of 3D models for medical imaging.

**Model Genesiso超过了所有2D方法**。一种常见的处理有限医学成像数据的技术，是将3D数据重新表示为2D图像，然后对ImageNet预训练的模型进行精调。这种方法将训练样本增加了一个数量级，但牺牲了3D上下文的信息。Genesis Chest CT与这种事实上的2D标准的比较，是很有趣的。为这个目的，我们采用了一种从ImageNet预训练的模型训练的2D模型，用在了NCC，NCS，ECC模型中。2D表示是通过从体数据集中提取轴向的切片得到的。表4比较了2D和3D模型的结果。注意，对于3D模型的结果是与表2中给出的结果是一样的。我们的统计分析证明了，从Genesis Chest CT训练得到的3D模型，明显超过了从ImageNet中训练得到的2D模型，取得了更高的平均性能以及更低的标准差（见表4和附录H）。但是，对于从头训练的模型来说，并没有这个结论，从头训练的3D模型，在三个目标模型的两个中超过了2D模型，同时很明显得到了更高的标准差。我们认为3D从头训练的模型的这种结果，是更多的模型参数，和目标任务中的有限取样大小，这共同阻碍了3D上下文的利用。实际上，从头训练的3D模型的不理想的性能，强调了Genesis Chest CT的有效性，给医学成像的3D模型以更强的性能。

Table 4: Comparison between 3D solutions and 2D slice-based solutions on three 3D target tasks. Training 3D models from scratch does not necessarily outperform the 2D counterparts (see NCC). However, training the same 3D models from Genesis Check CT outperforms (p<0.05) all 2D solutions, demonstrating the effectiveness of Genesis Chest CT in unlocking the power of 3D models.

Task | 2D Scratch | 2D ImageNet | 2D Genesis | 3D Scratch | 3D ImageNet | 3D Genesis | p-value
--- | --- | --- | --- | --- | --- | --- | ---
NCC | 96.03±0.86 | 97.79±0.71 | 97.45±0.61 | 94.25±5.07 | N/A | 98.20±0.51 | 0.0213
NCS | 70.48±1.07 | 72.39±0.77 | 72.20±0.67 | 74.05±1.97 | N/A | 77.62±0.64 | <1e-8
ECC | 71.27±4.64 | 78.61±3.73 | 78.58±3.67 | 79.99±8.06 | N/A | 88.04±1.40 | 5.50e-4

**Models Genesis (2D) offer equivalent performances to supervised pretrained models**. To compare our self-supervised approaches with those supervised pre-training from ImageNet [1], we deliberately downgrade our Models Genesis to 2D versions: Genesis Chest CT 2D and Genesis Chest X-ray (2D) (see visualization of Genesis 2D in Appendix Secs. F—G). The statistical analysis in Fig. 2 suggests that the downgraded Models Genesis 2D offer equivalent performance to state-of-the-art fine-tuning from ImageNet within modality, outperforming random initialization by a large margin, which is a significant achievement because ours comes at zero annotation cost. Meanwhile, the downgraded Models Genesis 2D are fairly robust in cross-domain transfer learning, although they tend to underperform when domain distance is large, which suggests same-domain transfer learning should be preferred where possible in medical imaging. For 3D applications, we also examine the effectiveness of fine-tuning from NiftyNet, which is not designed for transfer learning but is the only available supervised pre-trained 3D model. Compared with training from scratch, fine-tuning NiftyNet suffers 3.37, 0.18, and 0.03 points decrease for NCS, LCS, and BMS tasks, respectively (detailed in Appendix Sec. I), suggesting that strong supervision with limited annotated data cannot guarantee good transferability like ImageNet. Conversely, Models Genesis benefit from both large scale unlabeled datasets and dedicated proxy tasks which are essential for learning general-purpose visual representation.

**Model Genesis(2D)与有监督的预训练模型给出了相同的性能**。为比较我们的自监督方法与那些从ImageNet中的有监督预训练模型，我们故意将我们的Model Genesis降质到2D版：Genesis Chest CT 2D和Genesis Chest X-ray(2D)（在附录F-G中见Genesis 2D的可视化）。图2中的统计分析，说明降质的Models Genesis 2D，在相同的模态上，给出了与目前最好的从ImageNet上预训练的模型，相同的性能，超过了随机初始化很多，这是一个很大的进步，因为我们的模型没有标注代价。同时，降质的Models Genesis 2D在跨领域的迁移学习中也相对非常稳定，虽然在领域距离较大时，会表现相对较差一点，这说明在医学成像中，如果可能的话，应该首先倾向于相同领域的迁移学习。对于3D应用，我们还检查了从NiftyNet中精调的效果，这并不是为迁移学习设计的，但是唯一可用的有监督预训练3D模型。与从头训练相比，从NiftyNet精调，在NCS，LCS和BMS任务中，分别有3.37，0.18和0.03的性能损失（详见附录I），说明在有限的标注数据上的强监督，并不一定会保证很好的迁移性，就像ImageNet一样。相反的，Models Genesis从大规模无标注数据集和代理任务中受益，这对学习通用目标的视觉表示是很重要的。

Fig. 2: Comparison of 2D solutions on four 2D target tasks. To investigate the same- and cross-domain transferability of Models Genesis, we have trained Genesis Chest CT 2D using 2D axial slices from LUNA dataset (left panel), and Genesis Chest X-ray (2D) trained using radiographs from ChestX-ray8 dataset (right panel). In same-domain target tasks (NCC and NCS in the left panel and DXC in the right panel), Models Genesis 2D outperform training from scratch and offer equivalent performance to fine-tuning from ImageNet. While in cross-domain target tasks (DXC and IUC in the left panel; NCS and IUC in the right panel), Models Genesis 2D also produce fairly robust performance.

## 4 Conclusion and Future Work

A key contribution of ours is a collection of generic source models, nicknamed Models Genesis, built directly from unlabeled 3D image data with our novel unified self-supervised method, for generating powerful application-specific target models through transfer learning. While our empirical results are strong, surpassing state-of-the-art performances in most of the applications, an important future work is to extend our Models Genesis to modality-oriented models, such as Genesis MRI and Genesis Ultrasound, as well as organ-oriented models, such as Genesis Brain and Genesis Heart. In fact, we envision that Models Genesis may serve as a primary source of transfer learning for 3D medical imaging applications, in particular, with limited annotated data. To benefit the research community, we make the development of Models Genesis open science, releasing our codes and models to the public, and inviting researchers around the world to contribute to this effort. We hope that our collective efforts will lead to the Holy Grail of Models Genesis, effective across diseases, organs, and modalities.

我们的一个关键贡献，是一个通用源模型集合，称为Models Genesis，从无标注的3D图像数据中，使用我们的新型统一自监督方法得到，可以通过迁移学习生成强大的应用相关的目标模型。我们的经验结果是很强的，在大多数应用中超过了目前最好的性能，一个重要的未来工作是将我们的Models Genesis拓展到模态向的模型中，比如Genesis MRI和Genesis Ultrasound，以及器官向的模型，如Genesis Brain和Genesis Heart。实际上，我们认为，Model Genesis可以在3D医学成像应用中，作为一个基本的模型源，特别是在有限的标注数据中。为使研究团体受益，我们将Models Genesis开源了，放出了我们的代码和模型给公众，并邀请世界范围内的研究者为这个努力做贡献。我们希望，我们的集体努力会带来Models Genesis的集大成，在不同疾病、器官和模态中都可以有效。